{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Convolucionadas (RNC)  \n",
    "\n",
    "# Modelos Pre-entrenados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autor\n",
    "\n",
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "1.  Ian Goodfellow, Yosua Bengio and Aaron Courville, *Deep Learning*, MIT press, 2016.\n",
    "2. Vincent Doumolin and Francesco Visin, *A guide to convolution arithmetic for deep learning*, ArXiv:1603.07285v2, 2018\n",
    "3. https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "## Visualizando capas convolucionales\n",
    "\n",
    "Los modelos de redes neuronales generalmente se denominan opacos. Esto significa que son pobres para explicar la razón por la cual se tomó una decisión o predicción específica.\n",
    "\n",
    "Las redes neuronales convolucionales están diseñadas para trabajar con datos de imágenes, y su estructura y función sugieren que debería ser menos inescrutable que otros tipos de redes neuronales.\n",
    "\n",
    "Específicamente, los modelos se componen de pequeños filtros lineales y el resultado de aplicar filtros llamados mapas de activación, o más generalmente, mapas de características.\n",
    "\n",
    "Se pueden visualizar tanto filtros como mapas de características.\n",
    "\n",
    "Por ejemplo, podemos diseñar y comprender pequeños filtros, como detectores de línea. Quizás visualizar los filtros dentro de una red neuronal convolucional aprendida pueda proporcionar una idea de cómo funciona el modelo.\n",
    "\n",
    "Los mapas de características que resultan de aplicar filtros a las imágenes de entrada y a los mapas de características de las capas anteriores podrían proporcionar una idea de la representación interna que el modelo tiene de una entrada específica en un punto dado del modelo.\n",
    "\n",
    "Exploraremos ambos enfoques para visualizar una red neuronal convolucional en este tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuestra imagen\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"./Imagenes/bird.jpg\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">bird.jpg</p>\n",
    "</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo VGG preajustado\n",
    "\n",
    "Necesitamos un modelo para visualizar.\n",
    "\n",
    "En lugar de ajustar un modelo desde cero, podemos usar un modelo de clasificación de imágenes previo y de última generación.\n",
    "\n",
    "Keras proporciona muchos ejemplos de modelos de clasificación de imágenes de alto rendimiento desarrollados por diferentes grupos de investigación para el desafío de reconocimiento visual a gran escala ImageNet, o ILSVRC. Un ejemplo es el modelo VGG-16 que logró los mejores resultados en la competencia de 2014.\n",
    "\n",
    "Este es un buen modelo para usar para la visualización porque tiene una estructura uniforme simple de capas convolucionales y de agrupación ordenadas en serie, es profunda con 16 capas aprendidas y funcionó muy bien, lo que significa que los filtros y los mapas de características resultantes capturarán características útiles . Para obtener más información sobre este modelo, consulte el documento de 2015 [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/pdf/1409.1556.pdf).\n",
    "\n",
    "Podemos cargar y resumir el [modelo VGG16](https://keras.io/api/applications/) con solo unas pocas líneas de código; por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = VGG16()\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cómo visualizar filtros\n",
    "\n",
    "Quizás la visualización más simple de realizar es trazar los filtros aprendidos directamente.\n",
    "\n",
    "En la terminología de la red neuronal, los filtros aprendidos son simplemente pesos, pero debido a la estructura bidimensional especializada de los filtros, los valores de peso tienen una relación espacial entre sí y trazar cada filtro como una imagen bidimensional es significativo (o podría ser).\n",
    "\n",
    "El primer paso es revisar los filtros en el modelo, para ver con qué tenemos que trabajar.\n",
    "\n",
    "El resumen del modelo impreso en la sección anterior resume la forma de salida de cada capa, p. la forma de los mapas de características resultantes. No da ninguna idea de la forma de los filtros (pesos) en la red, solo el número total de pesos por capa.\n",
    "\n",
    "Podemos acceder a todas las capas del modelo a través de la propiedad model.layers.\n",
    "\n",
    "Cada capa tiene una propiedad layer.name, donde las capas convolucionales tienen una convolución de nomenclatura como el bloque # _conv #, donde ‘#‘ es un número entero. Por lo tanto, podemos verificar el nombre de cada capa y omitir cualquiera que no contenga la cadena ‘conv‘.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1_conv1 (3, 3, 3, 64)\n",
      "block1_conv2 (3, 3, 64, 64)\n",
      "block2_conv1 (3, 3, 64, 128)\n",
      "block2_conv2 (3, 3, 128, 128)\n",
      "block3_conv1 (3, 3, 128, 256)\n",
      "block3_conv2 (3, 3, 256, 256)\n",
      "block3_conv3 (3, 3, 256, 256)\n",
      "block4_conv1 (3, 3, 256, 512)\n",
      "block4_conv2 (3, 3, 512, 512)\n",
      "block4_conv3 (3, 3, 512, 512)\n",
      "block5_conv1 (3, 3, 512, 512)\n",
      "block5_conv2 (3, 3, 512, 512)\n",
      "block5_conv3 (3, 3, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "# summarize filters in each convolutional layer\n",
    "# summarize filter shapes\n",
    "for layer in model.layers:\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    # get filter weights\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recupera pesos de la primera capa \n",
    "\n",
    "Mostramos gráficamente los filtros de la primera capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve weights from the second hidden layer\n",
    "filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "\n",
    "# plot first few filters\n",
    "n_filters, ix = 6, 1\n",
    "for i in range(n_filters):\n",
    "\t# get the filter\n",
    "\tf = filters[:, :, :, i]\n",
    "\t# plot each channel separately\n",
    "\tfor j in range(3):\n",
    "\t\t# specify subplot and turn of axis\n",
    "\t\tax = pyplot.subplot(n_filters, 3, ix)\n",
    "\t\tax.set_xticks([])\n",
    "\t\tax.set_yticks([])\n",
    "\t\t# plot filter channel in grayscale\n",
    "\t\tpyplot.imshow(f[:, :, j], cmap='gray')\n",
    "\t\tix += 1\n",
    "# show the figure\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cómo visualizar mapas de características (feature maps)\n",
    "\n",
    "Los mapas de activación, llamados mapas de características, capturan el resultado de aplicar los filtros a la entrada, como la imagen de entrada u otro mapa de características.\n",
    "\n",
    "La idea de visualizar un mapa de características para una imagen de entrada específica sería comprender qué características de la entrada se detectan o conservan en los mapas de características. La expectativa sería que los mapas de características cercanos a la entrada detecten detalles pequeños o de grano fino, mientras que los mapas de características cercanos a la salida del modelo capturan características más generales.\n",
    "\n",
    "Para explorar la visualización de mapas de características, necesitamos información para el modelo VGG16 que pueda usarse para crear activaciones. Utilizaremos una fotografía simple de un pájaro. Específicamente, un Robin, tomado por Chris Heald y liberado bajo una licencia permisiva.\n",
    "\n",
    "Descargue la fotografía y colóquela en su directorio de trabajo actual con el nombre de archivo. [](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/02/bird.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image with the required shape\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from matplotlib import pyplot\n",
    "from numpy import expand_dims\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = VGG16()\n",
    "# redefine model to output right after the first hidden layer\n",
    "ixs = [2, 5, 9, 13, 17]\n",
    "outputs = [model.layers[i].output for i in ixs]\n",
    "model = Model(inputs=model.inputs, outputs=outputs)\n",
    "# load the image with the required shape\n",
    "img = load_img('./Imagenes/bird.jpg', target_size=(224, 224))\n",
    "# convert the image to an array\n",
    "img = img_to_array(img)\n",
    "# expand dimensions so that it represents a single 'sample'\n",
    "img = expand_dims(img, axis=0)\n",
    "# prepare the image (e.g. scale pixel values for the vgg)\n",
    "img = preprocess_input(img)\n",
    "# get feature map for first hidden layer\n",
    "feature_maps = model.predict(img)\n",
    "# plot the output from each block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square = 8\n",
    "for fmap in feature_maps:\n",
    "    # plot all 64 maps in an 8x8 squares\n",
    "    ix = 1\n",
    "    for _ in range(square):\n",
    "        for _ in range(square):\n",
    "            # specify subplot and turn of axis\n",
    "            ax = pyplot.subplot(square, square, ix)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # plot filter channel in grayscale\n",
    "            pyplot.imshow(fmap[0, :, :, ix-1], cmap='gray')\n",
    "            ix += 1\n",
    "    # show the figure\n",
    "    pyplot.figure.figsize=[20.,20.]\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
