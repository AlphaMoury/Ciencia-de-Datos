{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto codificadores Variacionales (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de una red generativa adversaria es escencialmente aproximar la distribución de un modelo de clasificación a partir de la entrada.\n",
    "\n",
    "En el caso de la base de datos *mnist* el modelo GAN es una función \n",
    "\n",
    "$$\n",
    "g:[-1,1]^{d} \\to [0,1]^{k},\n",
    "$$\n",
    "\n",
    "en donde, si $\\mathbf{x}\\in [-1,1]^{d}$ $g(\\mathbf{x})$ es una imagen que el discriminador no puede clasificarcomo falsa.\n",
    "\n",
    "\n",
    "Un modelo GAN digamos $g$ es un  modelo generativo, debido a que a partir de un hipercubo, sobre el cual asumimos una distribuición uniforme,  es capaz de producir una imagen  que es reconocida  por un clasificador de manera correcta, como si fuera una imagen auténtica. \n",
    "\n",
    "Esto significa que si $X\\sim [-1,1]^{d}$, entonces $g(X) \\in \\mathcal{C}(c)$, en donde $\\mathcal{C}(c)$ es un sunconjunto de \n",
    "$[0,1]^{k}$ asociado a la categoría $c$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo autoenconder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, en un autoencoder, el encoder hace un sumergimiento (embedding) de  la entrada en $\\mathcal{R}^m$.\n",
    "\n",
    "En el caso de *mnist* el encoder (codficador) es una función \n",
    "\n",
    "$$\n",
    "f : [0,1]^{k} \\to \\mathcal{R}^m,\n",
    "$$\n",
    "\n",
    "en donde en principio $m<<k$.\n",
    "\n",
    "El decoder (decodificador) es ahora un modelo generativo que intenta recuperar a partir del código $f(\\mathcal{x})$ el objeto $\\mathcal{x}$. En realidad, el decoder hace un trabajo similar a un modelo GAN, pero desde la represetación latente intermedia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia Variacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El propósito de la inferencia variacional es aproximar una densidad haciendo una paso intermedio por un espacio de variables latentes.\n",
    "\n",
    "El proceso  puede ser imaginado de una forma análoga a la construcción de un codificador en variables latentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobre un problema de Medición de trazos latentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fijar ideas, supongamos que se tiene el resultado de una prueba académica aplicada a una muestra de personas. Más aún, supongamos que la codificación es binaria, en donde 1 (uno) codifica una respuesta correcta y 0 (cero) una respuesta incorrecta. \n",
    "\n",
    "Observe que la representación vectorial de la base de datos *mnist*, puede considerarse similar si cada pixel se codifica como cero o uno únicamente.\n",
    "\n",
    "Entonces a la entrada se tienen vectores binarios  $\\mathbf{x}$. de tamaño fijo, digamos $d=100$. Se busca entonces una respresentación de estos vectores en un espacio Euclideano de dimensión $m$, en donde $m<<d$. \n",
    "\n",
    "Los expertos utilizan distintas técnicas como por ejemplo el empleo del análisis de componentes principales (ACP) o la $q$-dimensión, para detectar la dimensión $m$.\n",
    "\n",
    "El objetivo central de la medición es justamente la obtención de los vectores latentes que denotaremos $\\mathbf{z}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planteamiento del problema variacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que $p_{\\theta}(\\mathbf{x})$ es la densidad asociada a un vector aleatorio de respuesta. El problema estadístico en principio es estimar el parámetro $\\theta$ que indexa a la distribución.\n",
    "\n",
    "Si se asume que $\\mathbf{z}$ es el vector latente asociado a $\\mathbf{x}$  se tiene que\n",
    "\n",
    "$$\n",
    "P_{\\theta}(\\mathbf{x}) = \\int p_{\\theta}(\\mathbf{x},\\mathbf{z})d\\mathbf{z}.\n",
    "$$ \n",
    "\n",
    "A partir de esta ecuación se puede escribir que \n",
    "\n",
    "\n",
    "$$\n",
    "P_{\\theta}(\\mathbf{x})  = \\int P_{\\theta}(\\mathbf{x}|\\mathbf{z})P(\\mathbf{z})d\\mathbf{z},\n",
    "$$\n",
    "\n",
    "en donde $P(\\mathbf{z})$ es la distribución marginal del vector latente $\\mathbf{z}$. Esta expresión muestra el modelo generativo en el problema. Observese que una muestra de la distribución $P_{\\theta}(\\mathbf{x})$ puede ser obtenido como sigue:\n",
    "\n",
    "1. Genere una muestra $\\mathbf{z}\\sim P(\\mathbf{z})$\n",
    "2. Genere una muestra de $P_{\\theta}(\\mathbf{x}|\\mathbf{z})$.\n",
    "\n",
    "El problema es que en general $P_{\\theta}(\\mathbf{x}|\\mathbf{z})$ es intratable, en el sentido que por un lado la integral no puede obtenerse de forma directa y muestras de $P_{\\theta}(\\mathbf{x}|\\mathbf{z})$  tampoco se obtienen directamente, dado que precisamente se desconoce el parámetro $\\theta$. Obviamente al comienzo la marginal $P(\\mathbf{z})$ es desconocida\n",
    "\n",
    "Observe que adicionalmente \n",
    "\n",
    "$$\n",
    "p_{\\theta}(\\mathbf{x})= P(\\mathbf{x})\\int P_{\\theta}(\\mathbf{z}|\\mathbf{x})d\\mathbf{z}.\n",
    "$$\n",
    "\n",
    " El problema para determinar $P_{\\theta}(\\mathbf{x})$ es que en general $P_{\\theta}(\\mathbf{z}|\\mathbf{x})$ también es intratable.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproximación Variacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el propósito de convertir $p_{\\theta}(\\mathbf{z}|\\mathbf{x})$ en una función de densidad tratable la solución propuesta desde la inferencia variacional es la introducción de una densidad aproximada $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ de tal manera que\n",
    "\n",
    "$$\n",
    "Q_{\\phi}(\\mathbf{z}|\\mathbf{x}) \\approx P_{\\theta}(\\mathbf{z}|\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "La densidad $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ se escoge en una familia de distribuciones tratables indexadas por $\\phi$. Es común escoger $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ en la familia normal multivariada. Eso haremos en esta lección. \n",
    "\n",
    "Entonces para cada $\\mathbf{x}$ tendremos que\n",
    "\n",
    "$$\n",
    "Q_{\\phi}(\\mathbf{z}|\\mathbf{x}) = \\mathcal{N}(\\mathbf{z}; \\boldsymbol{\\mu}(\\mathbf{x}), \\text{diag}(\\boldsymbol{\\sigma}(\\mathbf{x})^2)\n",
    "$$\n",
    "\n",
    "$\\boldsymbol{\\mu}(\\mathbf{x})$ es elvector de medias (condicionadas a la entrada) y $(\\boldsymbol{\\sigma}(\\mathbf{x})$ es un vector de desviaciones estándar. Como la matriz de covarianza es diagonal, se está asumiendo que las componentes del vector $\\mathbf{z}$ son condicionalmente independientes, dado el vector de entrada $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divergencia Kullback-Leibler (KL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se ha definido la familia de disgtribuciones a partir de la cual se obtendrá la aproximación $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ el siguiente paso es deicidir como medir la proximidad o la discrepancia de la densidad aproximante con la densidad original. La solución sugerida desde la inferencia variacional es usar la divergencia KL, la cual se define por\n",
    "\n",
    "$$\n",
    "D_{KL}(Q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z}|\\mathbf{x})\\right)  = \\mathbb{E}_{\\phi}(\\log Q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}) - \\log p_{\\theta}(\\mathbf{z}|\\mathbf{x})\\right)).\n",
    "$$\n",
    "\n",
    "El símbolo $\\mathbb{E}_{\\phi}$ indica que la esperanza es con respecto a la densidad $Q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cota inferior de la evidencia (ELBO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo en la inferencia variacional es encontrar una densidad aproximante $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$  para la densidad $p_{\\theta}(\\mathbf{z}|\\mathbf{x})$ utilizando como métrica la divergencia KL, que por cierto no es una distancia, dado que no es simétrica.\n",
    "\n",
    "A partir del teorema de Bayes se obtiene que \n",
    "\n",
    "$$\n",
    "P_{\\theta}(\\mathbf{z}|\\mathbf{x})= \\frac{P_{\\theta}(\\mathbf{x}|\\mathbf{z})P_{\\theta}(\\mathbf{z})}{P_{\\theta}(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "Por lo que la divergencia KL se transforma en\n",
    "\n",
    "$$\n",
    "D_{KL}(Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z}|\\mathbf{x}))  = \\mathbb{E}_{\\phi}(\\log Q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}) - \\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})- \\log P_{\\theta}(\\mathbf{z})\\right)) + \\log P_{\\theta}(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "De donde se obtiene que\n",
    "\n",
    "\n",
    "$$\n",
    "\\log P_{\\theta}(\\mathbf{x}) - \n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z}|\\mathbf{x})] = \\mathbb{E}_{\\phi}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})]-\n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z})]\n",
    "$$\n",
    "\n",
    "Esta ecuación constituye lw núcleo de la inferencia variacional. El lado izquierdo de la ecuación  contiene el término $P_{\\theta}(\\mathbf{x})$ que se busca maximizar menos el error de la aproximación medido por $D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z}|\\mathbf{x})]$ que se espera que sea aproximadamente cero. \n",
    "\n",
    "\n",
    "Se sabe que la divergencia KL siempre es positiva, por lo que la parte izquierda de la ecuación se denomina como la cota inferior de la evidencia (**ELBO**) del inglés *evidence lower bound*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ecuación clave d ela inferencia variacional es dada por\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{ELBO}  & = \n",
    "\\log P_{\\theta}(\\mathbf{x}) - \n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z}|\\mathbf{x})] \\\\\n",
    "&= \\mathbb{E}_{\\phi}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})]-\n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z})]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "El proceso de optimización se basa en la segunda parte ecuación. El término $\\mathbb{E}_{\\phi}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})]$ corresponde al modelo generativo en el problema. La interpretación estadística de esta término es que el modelo generador toma muestras obtenidas de la salida del modelo de inferencia $P_{\\theta}(\\mathbf{z}|\\mathbf{x})$, el cual estamos aproximando con $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$. Es decir, se genera una muestra $\\mathbf{z} \\sim Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ y partir de esta se trata de reconstruir la entrada $\\mathbf{x}$.\n",
    "\n",
    "En el ejemplo propuesto, si se considera que se tiene vectores dicotómicos, se asume una distribución de Bernoulli para cada componente. Si las respuestas son condicionalmente independientes dado el vector $\\mathbf{z}$ entonces la función de pérdida es la entropía cruzada binaria $\\mathcal{L}_R$ dada por\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_R = \\frac{1}{d}\\sum_{j=1}^d x_j \\log p(\\mathbf{w}_j'\\mathbf{z} + b_j) + (1-x_j)\\log(1-p(\\mathbf{w}_j'\\mathbf{z} + b_j))\n",
    "$$\n",
    "\n",
    "\n",
    "El segundo término $D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z})]$ puede ser evaluado directamente. Como asumimos que $Q_{\\phi}$ es una distribución Gaussiana y si se tiene en cuenta que típicamente $P_{\\theta}(\\mathbf{z})= P(\\mathbf{z})=\\mathcal{N}(\\mathbf{0},\\mathbf{I})$, se obtiene que \n",
    "\n",
    "$$\n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z})]= \\frac{1}{2} \\sum_{j=1}^{d} (1+\\log(\\sigma_j)^2 - (\\mu_j)^2-(\\sigma_j)^2)\n",
    "$$\n",
    "\n",
    "Tanto $\\mu_j$ como $\\sigma_j$ son funciones de la entrada $\\mathbf{x}$ ue se estiman en el modelo de inferencia.\n",
    "\n",
    "Para minimizar $\\mathcal{L}_{KL} =D_{KL}$, se requiere que $\\mu_j\\to 0$ y $\\sigma_j\\to 1$.\n",
    "\n",
    "En resumen para el problema de inferencia variacional la función de pérdida es dada por\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{VAE} = \\mathcal{L}_{R} + \\mathcal{L}_{KL}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Variacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo del codificador en un autoencoder variacional es aproximar $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ mediante una red neuronal profunda.\n",
    "\n",
    "Tanto la media $\\boldsymbol{\\mu}(\\mathbf{x})$ como el vector de desviaciones  estándar $\\boldsymbol{\\sigma}(\\mathbf{x})$ son estimados por la red neuronal codificadora (encoder).\n",
    "\n",
    "El decodificador toma muestras latentes $\\mathbf{z}$ con el propósito de reconstruir la entrada como $\\tilde{\\mathbf{x}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El truco de la reparametrización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Los gradientes de propagación hacia atrás no pueden pasar por el bloque de muestreo estocástico. Si bien está bien tener entradas estocásticas para redes neuronales, no es posible los gradientes para pasar por una capa estocástica. La solución a este problema es eliminar el proceso de muestreo como entrada, como se muestra en el lado derecho de la siguiente figura. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"./Imagenes/reparametrizacion_truco.png\" width=\"500\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Reparametrización del Autoencoder Variacional </p>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La muestra  se calcula como:\n",
    "\n",
    "$$\n",
    "\\text{Muestra} = \\boldsymbol{\\mu} + \\boldsymbol{\\epsilon} \\odot  \\boldsymbol{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# install the nb extensions\n",
    "# conda install -c conda-forge jupyter_contrib_nbextensions \n",
    "#\n",
    "# enable the extensions\n",
    "#jupyter contrib nbextension install --user\n",
    "#jupyter nbextension enable equation-numbering/main\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n",
       "\n",
       "MathJax.Hub.Queue(\n",
       "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
       "  [\"PreProcess\", MathJax.Hub],\n",
       "  [\"Reprocess\", MathJax.Hub]\n",
       ");\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});\n",
    "\n",
    "MathJax.Hub.Queue(\n",
    "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
    "  [\"PreProcess\", MathJax.Hub],\n",
    "  [\"Reprocess\", MathJax.Hub]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Variacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Example of VAE on MNIST dataset using MLP\n",
    "\n",
    "The VAE has a modular design. The encoder, decoder and VAE\n",
    "are 3 models that share weights. After training the VAE model,\n",
    "the encoder can be used to  generate latent vectors.\n",
    "The decoder can be used to generate MNIST digits by sampling the\n",
    "latent vector from a Gaussian distribution with mean=0 and std=1.\n",
    "\n",
    "# Reference\n",
    "\n",
    "[1] Kingma, Diederik P., and Max Welling.\n",
    "\"Auto-encoding variational bayes.\"\n",
    "https://arxiv.org/abs/1312.6114\n",
    "'''\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling \n",
    "        fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    # K is the keras backend\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_mnist\"):\n",
    "    \"\"\"Plots labels and MNIST digits as function \n",
    "        of 2-dim latent vector\n",
    "\n",
    "    # Arguments:\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    x_test, y_test = data\n",
    "    xmin = ymin = -4\n",
    "    xmax = ymax = +4\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z, _, _ = encoder.predict(x_test,\n",
    "                              batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # axes x and y ranges\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([xmin,xmax])\n",
    "    axes.set_ylim([ymin,ymax])\n",
    "\n",
    "    # subsample to reduce density of points on the plot\n",
    "    z = z[0::2]\n",
    "    y_test = y_test[0::2]\n",
    "    plt.scatter(z[:, 0], z[:, 1], marker=\"\")\n",
    "    for i, digit in enumerate(y_test):\n",
    "        axes.annotate(digit, (z[i, 0], z[i, 1]))\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
    "    # display a 30x30 2D manifold of digits\n",
    "    n = 30\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-4, 4, n)\n",
    "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "image_size = x_train.shape[1]\n",
    "original_dim = image_size * image_size\n",
    "x_train = np.reshape(x_train, [-1, original_dim])\n",
    "x_test = np.reshape(x_test, [-1, original_dim])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 512\n",
    "batch_size = 128\n",
    "latent_dim = 2\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary \n",
    "# with the TensorFlow backend\n",
    "z = Lambda(sampling,\n",
    "           output_shape=(latent_dim,), \n",
    "           name='z')([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "plot_model(encoder,\n",
    "           to_file='vae_mlp_encoder.png',\n",
    "           show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "plot_model(decoder,\n",
    "           to_file='vae_mlp_decoder.png', \n",
    "           show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    help_ = \"Load tf model trained weights\"\n",
    "    parser.add_argument(\"-w\", \"--weights\", help=help_)\n",
    "    help_ = \"Use binary cross entropy instead of mse (default)\"\n",
    "    parser.add_argument(\"--bce\", help=help_, action='store_true')\n",
    "    args = parser.parse_args()\n",
    "    models = (encoder, decoder)\n",
    "    data = (x_test, y_test)\n",
    "\n",
    "    # VAE loss = mse_loss or xent_loss + kl_loss\n",
    "    if args.bce:\n",
    "        reconstruction_loss = binary_crossentropy(inputs,\n",
    "                                                  outputs)\n",
    "    else:\n",
    "        reconstruction_loss = mse(inputs, outputs)\n",
    "\n",
    "    reconstruction_loss *= original_dim\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer='adam')\n",
    "    vae.summary()\n",
    "    plot_model(vae,\n",
    "               to_file='vae_mlp.png',\n",
    "               show_shapes=True)\n",
    "\n",
    "    save_dir = \"vae_mlp_weights\"\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    if args.weights:\n",
    "        filepath = os.path.join(save_dir, args.weights)\n",
    "        vae = vae.load_weights(filepath)\n",
    "    else:\n",
    "        # train the autoencoder\n",
    "        vae.fit(x_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_data=(x_test, None))\n",
    "        filepath = os.path.join(save_dir, 'vae_mlp_mnist.tf')\n",
    "        vae.save_weights(filepath)\n",
    "\n",
    "    plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=batch_size,\n",
    "                 model_name=\"vae_mlp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
