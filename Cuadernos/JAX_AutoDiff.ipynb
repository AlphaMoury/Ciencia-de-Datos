{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'><span class=\"header-section-number\"> </span>Diferenciación Automática con JAX <br/>Introducción</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 1 </span> Introducción </h2>\n",
    "\n",
    "Con su versión actualizada de [Autograd](https://github.com/hips/autograd), [JAX](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html) puede diferenciar automáticamente el código nativo de Python y NumPy. Puede derivarse través de un gran subconjunto de características de Python, incluidos bucles, ifs, recursión y clousures, e incluso puede tomar derivadas de derivadas de derivadas. Admite la diferenciación tanto en modo inverso como en modo directo, y los dos pueden componerse arbitrariamente en cualquier orden.\n",
    "\n",
    "Lo nuevo es que JAX usa [XLA](https://www.tensorflow.org/xla) para compilar y ejecutar su código NumPy en aceleradores, como GPU y TPU. La compilación ocurre de forma predeterminada, con las llamadas de la biblioteca compiladas y ejecutadas justo a tiempo. Pero JAX incluso le permite compilar justo a tiempo sus propias funciones de Python en núcleos optimizados para XLA utilizando una API de una función. La compilación y la diferenciación automática se pueden componer de forma arbitraria, por lo que puede expresar algoritmos sofisticados y obtener el máximo rendimiento sin tener que abandonar Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade jax jaxlib \n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 2 </span> Gradientes </h2>\n",
    "\n",
    "Podemos diferenciar una función con *grad*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.41997433, dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain its gradient function\n",
    "grad_tanh = grad(np.tanh)\n",
    "grad_tanh(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*grad* toma una función y regresa una función. Supongamos que *f* es una función Python que evalua una función matemática. Entonces *grad(f)* es una función Python que evalua $\\nabla f$. Así *grad(f)(x)* evalua $\\nabla f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.070650816\n",
      "-0.13621867\n",
      "0.25265405\n"
     ]
    }
   ],
   "source": [
    "x = 2.0\n",
    "print(grad(np.tanh)(x))\n",
    "print(grad(grad(np.tanh))(x))\n",
    "print(grad(grad(grad(np.tanh)))(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41997433 0.07065082]\n"
     ]
    }
   ],
   "source": [
    "# vmap vectorize the function\n",
    "gr_v = vmap(grad(np.tanh))\n",
    "x = np.array([1.,2.0])\n",
    "print(gr_v(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.7615942, 0.9640276], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \n",
    "# import elementwise_grad for functions that vectorix¿ze over inputs\n",
    "from __future__ import absolute_import\n",
    "import autograd.numpy as onp\n",
    "from autograd import elementwise_grad as egrad\n",
    "import matplotlib.pyplot as plt\n",
    "x = onp.linspace(-7,7,200)\n",
    "g1 = jit(vmap(grad(np.tanh))) # first derivative\n",
    "g2 = jit(vmap(grad(grad((np.tanh))))) # second derivative\n",
    "g3 = jit(vmap(grad(grad(grad((np.tanh)))))) # third derivative\n",
    "g4 = jit(vmap(grad(grad(grad(grad((np.tanh))))))) # fourth derivative\n",
    "\n",
    "plt.plot(x, np.tanh(x),\n",
    "         x, g1(x),\n",
    "         x, g2(x),\n",
    "         x, g3(x),\n",
    "         x, g4(x)\n",
    "        )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 3 </span> Modelo de Regresión Logística desde cero usando grad </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 0.5*(np.tanh(x/2)+1)\n",
    "# seems more stable than  1.0/(1+np.exp(-x))\n",
    "\n",
    "# outputs probability of a label being true\n",
    "def predict(W,b,inputs):\n",
    "    return sigmoid(np.dot(inputs,W)+b)\n",
    "\n",
    "# Build a toy dataset\n",
    "inputs = np.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "targets = np.array([True, True, False, True])\n",
    "\n",
    "# training loss: -log likelihood of trainig examples\n",
    "def loss(W,b,x,y):\n",
    "    preds = predict(W,b,x)\n",
    "    label_probs = preds*y + (1-preds)*(1-y)\n",
    "    return -np.sum(np.log(label_probs))\n",
    "\n",
    "# initialize coefficients\n",
    "key, W_key, b_key = random.split(key,3)\n",
    "W = random.normal(key, (3,))\n",
    "b = random.normal(key,())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la funcion *grad* con sus argumentos  para diferenciar la función con respecto a sus parámetros ṕosicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile with jit\n",
    "# argsnums define positional params to derive with respect to\n",
    "grad_loss = jit(grad(loss,argnums=(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_grad =  [-0.13325673  0.7287398  -1.7607927 ]\n",
      "b_grad =  0.022453208\n"
     ]
    }
   ],
   "source": [
    "W_grad, b_grad = grad_loss(W,b,inputs, targets)\n",
    "print(\"W_grad = \", W_grad)\n",
    "print(\"b_grad = \", b_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de entrenamiento de un modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(W,b,x,y, lr= 0.12):\n",
    "    gradient = grad_loss(W,b,inputs,targets) \n",
    "    W_grad, b_grad = grad_loss(W,b,inputs,targets)\n",
    "    W -= W_grad*lr\n",
    "    b -= b_grad*lr\n",
    "    return(W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss 2.3193581104278564\n",
      "Epoch 1: train loss 2.01922345161438\n",
      "Epoch 2: train loss 1.7796587944030762\n",
      "Epoch 3: train loss 1.5825984477996826\n",
      "Epoch 4: train loss 1.4183536767959595\n",
      "Epoch 5: train loss 1.2804994583129883\n",
      "Epoch 6: train loss 1.1641706228256226\n",
      "Epoch 7: train loss 1.0654593706130981\n",
      "Epoch 8: train loss 0.9811764359474182\n",
      "Epoch 9: train loss 0.9087210297584534\n",
      "Epoch 10: train loss 0.8459861278533936\n",
      "Epoch 11: train loss 0.7912724614143372\n",
      "Epoch 12: train loss 0.7432132959365845\n",
      "Epoch 13: train loss 0.7007092833518982\n",
      "Epoch 14: train loss 0.6628734469413757\n",
      "Epoch 15: train loss 0.628989040851593\n",
      "Epoch 16: train loss 0.5984709858894348\n",
      "Epoch 17: train loss 0.5708418488502502\n",
      "Epoch 18: train loss 0.5457080006599426\n",
      "Epoch 19: train loss 0.522742509841919\n"
     ]
    }
   ],
   "source": [
    "#    \n",
    "weights, biases = [], []\n",
    "train_loss= []\n",
    "epochs = 20\n",
    "\n",
    "train_loss.append(loss(W,b,inputs,targets))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    W,b = train(W,b,inputs, targets)\n",
    "    weights.append(W)\n",
    "    biases.append(b)\n",
    "    losss = loss(W,b,inputs,targets)\n",
    "    train_loss.append(losss)\n",
    "    print(f\"Epoch {epoch}: train loss {losss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "[ 0.5526737  -0.0814647  -0.11806437]\n",
      "[ 0.55927765 -0.13807121  0.06845167]\n",
      "[ 0.5614956  -0.17740442  0.2367465 ]\n",
      "[ 0.561902   -0.2063056   0.39015874]\n",
      "[ 0.5618833  -0.22850716  0.53068244]\n",
      "[ 0.5621856  -0.24620296  0.6597552 ]\n",
      "[ 0.56318307 -0.2607605   0.77857214]\n",
      "[ 0.56502855 -0.27307138  0.888201  ]\n",
      "[ 0.5677453 -0.2837374  0.9896157]\n",
      "[ 0.57128537 -0.29317585  1.0837021 ]\n",
      "[ 0.5755653  -0.30168238  1.1712576 ]\n",
      "[ 0.58048826 -0.30947018  1.252992  ]\n",
      "[ 0.5859567 -0.3166952  1.3295317]\n",
      "[ 0.59187865 -0.32347307  1.4014258 ]\n",
      "[ 0.59817106 -0.32989037  1.4691548 ]\n",
      "[ 0.60476077 -0.33601263  1.5331378 ]\n",
      "[ 0.61158454 -0.34188995  1.5937407 ]\n",
      "[ 0.6185881  -0.34756085  1.6512834 ]\n",
      "[ 0.6257253 -0.3530553  1.706046 ]\n",
      "[ 0.63295746 -0.3583968   1.7582744 ]\n",
      "biases\n",
      "0.8808514\n",
      "0.8669749\n",
      "0.8486012\n",
      "0.8292053\n",
      "0.81065005\n",
      "0.7939027\n",
      "0.7793964\n",
      "0.7672425\n",
      "0.7573656\n",
      "0.74959135\n",
      "0.7437017\n",
      "0.7394684\n",
      "0.73667145\n",
      "0.73510873\n",
      "0.7345997\n",
      "0.73498607\n",
      "0.7361306\n",
      "0.7379152\n",
      "0.74023885\n",
      "0.7430152\n"
     ]
    }
   ],
   "source": [
    "print('weights')\n",
    "for weight in weights:\n",
    "    print(weight)\n",
    "print('biases')\n",
    "for bias in biases:\n",
    "    print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este API de *grad* tiene una correspondencia directa con la excelente notación en el clásico *Cálculo sobre Variedades* (1965), también utilizado en la Estructura e interpretación de la mecánica clásica* (2015) de Sussman y Wisdom's (2015) y su *Geometría diferencial funcional* (2013). Ambos libros son de acceso abierto. Consulte en particular la sección \"Prólogo\" de Geometría diferencial funcional para una defensa de esta notación.\n",
    "\n",
    "Esencialmente, usando los argnums de una función *f* se obtienen derivadas parciales. Por ejemplo graf(f,i) evalua $\\partial_i f$. Pro ejemplo $\\partial loss/\\partial W$ se denota se escribe $\\partial_0 loss$ y se escribe con *grad* como"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06078603  0.04339207 -0.41592562]\n"
     ]
    }
   ],
   "source": [
    "print(grad(loss,0)(W,b,inputs,targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 4 </span> Diferenciación con respecto a listas anidadas, tuplas y dicts </h2>\n",
    "\n",
    "La diferenciación con respecto a los contenedores estándar de Python simplemente funciona, así que use tuplas, listas y dictados (y anidamiento arbitrario) como desee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss2(params_dict,x,y):\n",
    "    preds = predict(params_dict['W'], params_dict['b'], x)\n",
    "    label_probs = preds*y + (1-preds)*(1-y)\n",
    "    return -np.sum(np.log(label_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W': DeviceArray([-0.06078603,  0.04339207, -0.41592562], dtype=float32), 'b': DeviceArray(-0.02629587, dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print(grad(loss2)({'W':W, 'b':b},inputs,targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando el valor de la función y el gradiente con value_and_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value:  0.5227425\n",
      "gradient value:  (DeviceArray([-0.06078603,  0.04339207, -0.41592562], dtype=float32), DeviceArray(-0.02629587, dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "from jax import value_and_grad\n",
    "loss_val, Wb_grad = value_and_grad(loss,(0,1))(W,b,inputs, targets)\n",
    "print('loss value: ', loss_val)\n",
    "print('gradient value: ', Wb_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.test_util import check_grads\n",
    "def loss(W,b):\n",
    "    preds = predict(W,b,inputs)\n",
    "    label_probs = preds*targets + (1-preds)*(1-targets)\n",
    "    return -np.sum(np.log(label_probs))\n",
    "\n",
    "assert check_grads(loss, (W, b), order=2)  # check up to 2nd order derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
