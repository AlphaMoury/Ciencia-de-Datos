{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal desde Cero - Introducción a Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autor\n",
    "\n",
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "1.  Jeremy Howard, Rachel Thomas and Francisco Ingham, <a href=\"https://jaredwinick.github.io/what_is_tf_keras/\">What is tf.keras really?</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introducción \n",
    "\n",
    "Este tutorial está basado en el tutorial <a href=\"https://jaredwinick.github.io/what_is_tf_keras/\">What is tf.keras really?</a>  La lectura de datos no la haremos directamente de la URL usada por los autores, porque el archivo fue movido de allí. Usaremos los datos disponible en *tf.keras.dataset*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La base de datos MNIST \n",
    "\n",
    "El ejemplo que usaremos es la base de datos MNIST (Modified National Institute of Standards and Technology database), la cual está disponible libremente en diferentes sitios de internet, así como también desde los lenguajes de programación usados en el aprendizaje de máquinas. \n",
    "\n",
    "\n",
    "Esta es una base de datos de dígitos entre cero y uno digitalizados en escala de grises. Esto siginifica que cada punto de la imagen digitalizada es representada  mediante un byte. Esto siginifica que es posible representar 256 tonos de grises, con valores enteros entre 0 (blanco) hasta 255 (negro). Además, la base de datos contiene una etiqueta (label) para cada  imagen.\n",
    "\n",
    "La siguiente imagen tomada de Wikipedia, muestra algunas de las imagenes de la base de datos MNIST.\n",
    "  \n",
    "    \n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"./Imagenes/MnistExamples.png\" width=\"400\" height=\"200\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Imágenes de dígitos en la base de datos MNIST. Imagen tomada de Wikipedia</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Cada imagen tiene un tamaño de 28 x 28 pixeles, con lo cual cada una es representada por 784 números entre cero y 255. En el preprocesamiento las matrices de  28 x 28 números representando cada imagen son convertidas a arreglos unidimensionales de tamaño 784, colocandp una fila a continuación de la otra.  Este proceso es conocido como aplanamiento (flatened). \n",
    "\n",
    "Adicionalmente los arreglos son escalados dividiendo cada componente entre 255, para obtener valores entre cero y uno.\n",
    "\n",
    "Por otro lado, las etiquetas serán recodificadas usando la codificación One-Hot-Encodding (dummy encodding) o  codificación 0-1. Es decir que cada etiqueta es representada por un arreglo de diez dígitos binarios, de los cuales todos son cero excepto uno. Por ejemplo si la etiqueta es 3, la codificación One-Hot-Encodding  será *0010000000*.\n",
    "\n",
    "La base de datos MNIST tiene 60.000 imágenes marcadas para entrenamiento (training) y 10.000 para evaluación (testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de Tensorflow:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Versión de Tensorflow: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape= (60000, 28, 28)\n",
      "<class 'numpy.ndarray'>\n",
      "x_train.dtype uint8\n",
      "y_train.shape= (60000,)\n",
      "<class 'numpy.ndarray'>\n",
      "y_train.dtype uint8\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_valid, y_valid) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"x_train.shape=\",x_train.shape)\n",
    "print(type(x_train))\n",
    "print('x_train.dtype',x_train.dtype)\n",
    "\n",
    "print(\"y_train.shape=\",y_train.shape)\n",
    "print(type(y_train))\n",
    "print('y_train.dtype',y_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos datos tiene formato de arreglos numpy. Tensorflow utiliza el formato tf.Tensor, por lo que es necesario transformar los datos a dicho formato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape x_train =  (60000, 784)\n",
      "type x_train =  <dtype: 'float32'>\n",
      "Min x_train =  0.0\n",
      "Max x_train =  1.0\n",
      "type y_train =  <dtype: 'int32'>\n",
      "Min y_train =  0\n",
      "Max y_train =  9\n"
     ]
    }
   ],
   "source": [
    "# resize x\n",
    "x_train.resize((60000,784)) # Change shape and size of array in-place.\n",
    "x_valid.resize((10000,784))\n",
    "\n",
    "\n",
    "# transform x to [0,1] scale and create constant tensors  \n",
    "x_train = tf.constant(x_train/255., dtype= tf.float32)\n",
    "x_valid = tf.constant(x_valid/255., dtype= tf.float32)\n",
    "\n",
    "\n",
    "print(\"shape x_train = \",x_train.shape)\n",
    "print(\"type x_train = \",x_train.dtype)\n",
    "print(\"Min x_train = \",tf.reduce_min(x_train).numpy())\n",
    "print(\"Max x_train = \",tf.reduce_max(x_train).numpy())\n",
    "\n",
    "# transform y to int32 and create constant tensors\n",
    "y_train = tf.constant(y_train,dtype=tf.int32)\n",
    "y_valid = tf.constant(y_valid,dtype=tf.int32)\n",
    "\n",
    "print(\"type y_train = \",y_train.dtype)\n",
    "print(\"Min y_train = \",tf.reduce_min(y_train).numpy())\n",
    "print(\"Max y_train = \",tf.reduce_max(y_train).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos *tf.constant* para declarar tensores constantes, es decir cuyo valor no cmabia a lo largo del algoritmo.  Note que se ha dividido x_train y x_valid entre 255, para transformar los datos a escala $[0.0,1.0]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primera Imagen de la base de datos\n",
    "\n",
    "La siguiente imagen corresponde al primer registro de la base de datos MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333336 0.6862745  0.10196079 0.6509804  1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
      " 0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215687\n",
      " 0.93333334 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.9843137  0.3647059  0.32156864\n",
      " 0.32156864 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882354 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.7764706  0.7137255\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
      " 0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.6039216  0.99215686 0.3529412\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509807 0.99215686 0.74509805 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313726\n",
      " 0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13725491 0.94509804\n",
      " 0.88235295 0.627451   0.42352942 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764707 0.9411765  0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
      " 0.5882353  0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.3647059  0.9882353  0.99215686 0.73333335\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.9764706  0.99215686 0.9764706  0.2509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
      " 0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.5803922\n",
      " 0.8980392  0.99215686 0.99215686 0.99215686 0.98039216 0.7137255\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705883 0.8666667  0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.7882353  0.30588236 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058825\n",
      " 0.85882354 0.99215686 0.99215686 0.99215686 0.99215686 0.7647059\n",
      " 0.3137255  0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568628 0.6745098  0.8862745  0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156866 0.04313726 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333336 0.99215686\n",
      " 0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(x_train[0].numpy().reshape((28,28)),cmap=\"gray\")\n",
    "plt.show()\n",
    "print(x_train.shape)\n",
    "print(type(x_train))\n",
    "print(x_train[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diseño de la Red Neuronal Básica. Modelo Logístico Multinomial\n",
    "\n",
    "El propósito de la red es clasificar cada una de las imágenes en una de 10 clases. Cada clase representa a un dígito entre 0 y 9.\n",
    "\n",
    "En este experimento cada imagen será presentada a la entrada de la red como una tensor de una dimensión (1D) y de tamaño (shape) $28\\times 28 = 784$. Por lo tanto, la capa entrada se tendran 784 neuronas. La red no tendrá capas ocultas. La capa de salida tendrá 10 neuronas, debido a que hay 10 clases.\n",
    "\n",
    "El tipo de red será densa o completamente conectada, es  decir cada neurona de entrada está conectada con cada neurona de salida. \n",
    "\n",
    "Así, la matriz de pesos $\\mathbf{W}$ tendrá tamaño $784 \\times 10$ y el vector $\\mathbf{b}$ de interceptos (bias) será de un vector tamaño 10.\n",
    "\n",
    "\n",
    "En nuestro primer modelo, la función de activación de la capa de entrada será la identidad y en la capa de salida la función de activación será la función *softmax* que definimos abajo.\n",
    "\n",
    "La siguiente imagen ilustra el diseño (topología) de la primera red que constriomos desde cero. Esta red, es realmente un modelo logístico multinomial clásico.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"./Imagenes/ANN_mnist_748_10.jpg\" width=\"600\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Topología de  la  red neuronal. Modelo Logístico Multinomial </p>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La Red Neuronal desde cero (sin tf.keras) </h2>\n",
    "\n",
    "Vamos a crear primero un modelo, para lo cual solamente usaremos las operaciones de Tensorflow.  Los pesos son inicializados siguiendo la propuesta de <a href='http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf'>Gorot_Bengio </a>. Se generaran números aleatorios y se divide por $\\sqrt{n}$. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weights = tf.Variable(tf.random.normal((784,10))/math.sqrt(784), dtype= tf.float32)\n",
    "bias = tf.Variable(tf.zeros(10),tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos  *tf.Variable* para declarar tensores de tipo variable, es decir, cuyo contenido cambia a los largo de nuestro algortimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Racional\n",
    "\n",
    "Por  otro lado, observe que *weights* es una matriz de tamaño $784 \\times 10$ y *bias* es un vector de tamaño 10. Vamos a construir una red neuronal que tiene solamente dos capas: entrada con 784 neuronas y salida con 10 neuronas. Note que nuestros patrones de entrada son vectores de tamaño 748.\n",
    "\n",
    "La razón de tener 10 neuronas de salida es porque tenemos un problema de clasificación en 10 clases. Podemos imaginar intuitivamente es que construimos un modelo de regresión logística para cada una de las clases. Entonces a la salida esperamos tener la probabilidad que el patron de entrada pertenezca a esa clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferenciación Automática\n",
    "\n",
    "Tensorflow 2.0 incluye **diferenciación automática** para calcular gradientes automáticamente. Esto permite usar funciones estándar de Python como modelos para la redes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Activación softmax\n",
    "\n",
    "Vamos a crear un modelo lineal simple. Necesitaremos una función de activación, por lo que vamos a escribir la función *log_softmax*. La variable *eta* en la función, que corresponde al máximo de los valores, se introduce para tener una función muy robusta, desde el punto de vista computacional.\n",
    "\n",
    "Para un conjunto de valores $x_1,\\ldots,x_n$, la función softwax transforma estos valores en la escala (0,1), que pueden interpretarse como probabilidades. Cada componente de softmax es interpretada como la probabilidad que la imagen pertenezca a la clase representada por dicha componente.\n",
    "\n",
    "Matemáticamente se escribe\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "Prob[y=j|x] = \\frac{\\exp(x_j)}{\\sum_{k=1}^{n}\\exp(x_j)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Un cálculo muy estable computacionalmente de estos valores se obtiene de la siguente manera. Sea $\\eta = \\underset{i}{\\text{max}} \\hspace{2mm} \\{x_i\\}$. Es fácil verificar que\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "Prob[y=j|x] = \\frac{\\exp(x_j-\\eta)}{\\sum_{k=1}^{n}\\exp(x_k-\\eta)} = \\text{softmax}_j(x)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Observe que $\\log \\text{softmax}_j(x)  = x_j - \\eta - \\log(\\sum_{k=1}^n \\exp(x_k-\\eta))$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    eta = tf.math.reduce_max(x)\n",
    "    return x - eta - tf.math.log(tf.math.reduce_sum(tf.math.exp(x-eta), -1, keepdims=True))\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias) # z = x'W +b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda anterior, el símbolo @ representa la operación del producto punto. Llamaremos a nuestra función con un lote de datos (en este caso, 64 imágenes). Este es un paso hacia adelante (forward pass). Tenga en cuenta que nuestras predicciones no serán mejores que aleatorias en esta etapa, ya que comenzamos con pesos aleatorios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones para la primera imagen:  tf.Tensor(\n",
      "[-2.1273656 -2.1138806 -2.6305888 -2.537918  -2.732724  -1.9756156\n",
      " -2.4066586 -2.0980308 -2.3994372 -2.2855947], shape=(10,), dtype=float32) \n",
      " Tamaño de la matriz de predicciones de este lote:  (64, 10)\n"
     ]
    }
   ],
   "source": [
    "bs = 64 # batch size\n",
    "\n",
    "xb = x_train[0:bs]\n",
    "preds = model(xb) # predictions\n",
    "print('Predicciones para la primera imagen: ',preds[0],'\\n Tamaño de la matriz de predicciones de este lote: ', preds.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de pérdida\n",
    "\n",
    "Vamos a implementar la función menos log-verosimilitud, la cual usaremos como **función de pérdida** (loss function). Esta es la misma **entropía cruzada**(cross entropy, usando la codificación One-hot).\n",
    "\n",
    "\n",
    "Matemáticamente, la función de pérdida en esta caso es definida como sigue:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  El Modelo\n",
    "\n",
    "Aqui $\\mathbf{x}_i$ denota la observación $i$. \n",
    "\n",
    "$$\n",
    "\\pi_{ik} = Prob[\\mathbf{x} \\in \\mathcal{C}_k] = sofmax_k(\\mathbf{x}_i) = \\frac{\\exp(\\mathbf{x}_i'\\mathbf{w}_k + \\mathbf{b}_k)}{\\sum_{s=1}^K  \\exp (\\mathbf{x}_i'\\mathbf{w}_s+  \\mathbf{b}_s)} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Función de pérdida \n",
    "\n",
    "En esta sección $\\mathbf{W}$ es la martriz completa de pesos. Cada fila $\\mathbf{w}_k$ está asociada a la respectiva categoría $\\mathcal{C}_k$.\n",
    "\n",
    "Entropía cruzada: **-log verosimilitud** .  Aqui $\\mathbf{x}_i$ denota la observación $i$ y $y_i$ la respectiva etiqueta. Sea $\\chi_{ik}$ definda por\n",
    "\n",
    "$$\n",
    "\\chi_{ik} = \\begin{cases} 1, & \\text{ si } y_i = k\\\\\n",
    "0, & \\text{ en otro caso. }\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "loss(\\mathbf{W},\\mathbf{b}) = -\\frac{1}{N}  \\sum_i \\sum_k \\chi_{ik} \\log \\pi_{ik}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción\n",
    "\n",
    "$$\n",
    "\\tilde{y} = \\text{índice}(\\max_{k}{\\pi_k(x)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Precisión\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\text{accuracy} = \\frac{1}{N} \\sum_{i=1}^{N} 1_{y_i =\\tilde{y}_i}\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    indices = tf.stack([tf.range(input.shape[0], dtype=tf.int32),target], axis=1)\n",
    "    return tf.math.reduce_mean(-tf.gather_nd(input, indices))\n",
    "\n",
    "loss_func = nll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo del tensor indices\n",
    "import pandas as pd\n",
    "pattern = {'0': 0, '1': 1, '2':2, '3':3, '4':4}\n",
    "target = {'0': 3, '1': 0, '2':5, '3':7, '4':1}\n",
    "indices = pd.DataFrame({'pattern': pattern, 'target':target})\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre el código anterior. \n",
    "1. *tf.range(input.shape[0])*  genera un tensor con números en el rango entre 0 y 63, porque la forma (shape) es 64, el tamaño del batch.\n",
    "2. *tf.stack([tf.range(input.shape[0], dtype=tf.int32),target], axis=1)* crea un tensor bidimensional, en donde cada fila corresponde al índice del elemento que debe tomarse de cada fila de input para calcular la funcion de pérdida. Por ejemplo, si la fila 3 de índice es [3,4], significará que y_train[3] es 4, y ṕor tanto se requiere tomar el contenido de la cuarta posición de la entrada 3.\n",
    "3. tf.gather_nd(input, indices). Para cada fila de input toma el valor de input en la posición indicada en el índice, como se explica en el numeral 2.\n",
    "4. tf.math.reduce_mean(-tf.gather_nd(input, indices)), calcula el resumen. La media, que corresponde exáctamete a - log likelihood\n",
    "\n",
    "Veámos una ilustración de índices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pattern</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pattern  target\n",
       "0        0       3\n",
       "1        1       0\n",
       "2        2       5\n",
       "3        3       7\n",
       "4        4       1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ejemplo del tensor indices\n",
    "import pandas as pd\n",
    "pattern = {'0': 0, '1': 1, '2':2, '3':3, '4':4}\n",
    "target = {'0': 3, '1': 0, '2':5, '3':7, '4':1}\n",
    "indices = pd.DataFrame({'pattern': pattern, 'target':target})\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la columna pattern en esta ilustración contiene el índice del respectivo patrón de entrada. La columna target es el target asociado al respectivo patrón de entrada. Indices es esta matriz con las dos columnas.\n",
    "\n",
    "La última línea del código calcula la función de pérdida definida arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos ahora la **función de pérdida** para el modelo con datos aleatorios. Esto permitirá comparar las mejoras después del paso de propagacion hacia atrás  (backpropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.3694425, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds,yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También implementamos la función precisión (accuracy) de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out,yb):\n",
    "    preds = tf.dtypes.cast(tf.math.argmax(out,axis=1), tf.int32)   # posición del valor máximo en cada arreglo\n",
    "    return tf.math.reduce_mean(tf.dtypes.cast(preds==yb,tf.float16))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chequamos la precisión actual del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0625, shape=(), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(preds,yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de la Red\n",
    " \n",
    " Estamos listos para correr el ciclo de entranamiento. Para calcular los gradientes usaremos diferenciación automática, con la clase tf.GradientTape.\n",
    " \n",
    "- Seleccionamos un mini-batch de datos de tamaño *bs*\n",
    "- Bajo un contexto tf.GradientTape \n",
    " - Usamos el modelo para hacer predicciones\n",
    " - Calculamos la pérdida\n",
    "- Calculamos el gradientes de las operaciones registradas en el contexto de esta cinta (tape).\n",
    "\n",
    "Ahora usamos estos gradientes para actualizar los pesos (weights) y los desplazamientos (bias).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.2 # learning rate\n",
    "epochs = 2 # how many epochs to train for\n",
    "n = x_train.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs +1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        with tf.GradientTape() as t:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred,yb)\n",
    "        dW, dB = t.gradient(loss,[weights,bias])\n",
    "        weights.assign_sub(lr*dW)\n",
    "        bias.assign_sub(lr*dB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos creado y entrenado una red neuronal minimal( en este caso una regression logística multinomial, dado que no tiene capas ocultas) totalmwente desde cero (from scratch).\n",
    "\n",
    "Revisemos la pérdida y precisión del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  tf.Tensor(0.30663186, shape=(), dtype=float32)\n",
      "train accuracy:  tf.Tensor(0.911, shape=(), dtype=float16)\n",
      "ttest loss:  tf.Tensor(0.30020323, shape=(), dtype=float32)\n",
      "test accuracy:  tf.Tensor(0.9146, shape=(), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "# pérdida y precisión datos de entrenamiento\n",
    "print(\"train loss: \",loss_func(model(x_train),y_train))\n",
    "print(\"train accuracy: \",accuracy(model(x_train),y_train))\n",
    "\n",
    "# pérdida y precisión datos de validación\n",
    "print(\"test loss: \",loss_func(model(x_valid),y_valid))\n",
    "print(\"test accuracy: \",accuracy(model(x_valid),y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La Red Neuronal usando tf.keras\n",
    "\n",
    "Ahora refactorizaremos nuestro código, para que haga lo mismo que antes, solo que comenzaremos a aprovechar las clases *tf.keras* de TensorFlow para hacerlo más conciso y flexible. En cada paso a partir de aquí, deberíamos hacer que nuestro código sea más corto, más comprensible y / o más flexible.\n",
    "\n",
    "El primer y más fácil paso es acortar nuestro código reemplazando nuestras funciones de activación y pérdida escritas a mano con las de tf.keras.\n",
    "\n",
    "TensorFlow proporciona una función única función *tf.keras.losses.SparseCategoricalCrossentropy* que combina una activación softmax con una función de pérdida. Tenga en cuenta que usamos from_logit = True aquí porque no estamos pasando una distribución de probabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.09631616, shape=(), dtype=float32) tf.Tensor(0.9688, shape=(), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(yb,model(xb)), accuracy(model(xb),yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactorización usando tf.keras.Model \n",
    "\n",
    "A continuación, utilizaremos *tf.keras.Model* para un ciclo de entrenamiento más claro y conciso. Hacemos una subclase (heredamos) de la clase *tf.keras.Model* (que en sí mismo es una clase y puede realizar un seguimiento del estado). En este caso, queremos crear una clase que contenga nuestros pesos, sesgos y métodos para el paso adelante (forward pass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_logistic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Mnist_logistic,self).__init__()\n",
    "        self.w = tf.Variable(tf.random.normal((784,10)) /math.sqrt(784))\n",
    "        self.b = tf.Variable(tf.zeros(10))\n",
    "    \n",
    "    def call(self, xb):\n",
    "        return xb @ self.w + self.b\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ahora estamos usando un objeto en lugar de solo usar una función, primero tenemos que instanciar nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos calcular la pérdida de la misma manera que antes. Tenga en cuenta que los objetos *tf.keras.Model* se usan como si fueran funciones (es decir, son invocables), pero detrás de escena TensorFlow llamará a nuestro método *call* automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.3267632, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(yb,model(xb)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Anteriormente para nuestro ciclo de entrenamiento teníamos que actualizar los valores para cada parámetro por nombre, de esta manera:\n",
    "\n",
    "    weights.assign_sub (lr * dW)\n",
    "    bias.assign_sub (lr * dB)\n",
    "\n",
    "\n",
    "Ahora podemos aprovechar *model.trainable_variables* para hacer que esos pasos sean más concisos y menos propensos al error de olvidar algunos de nuestros parámetros, especialmente si teníamos un modelo más complicado.\n",
    "\n",
    "Envolveremos nuestro pequeño bucle de entrenamiento en una función de ajuste para poder ejecutarlo nuevamente más tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1)//bs +1):\n",
    "            start_i = i*bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            with tf.GradientTape() as t:\n",
    "                pred = model(xb)\n",
    "                loss = loss_func(yb, pred)\n",
    "                \n",
    "            gradients = t.gradient(loss, model.trainable_variables)\n",
    "            for variable, grad in zip(model.trainable_variables, gradients):\n",
    "                variable.assign_sub(lr * grad)\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chequeemos que la  pérdida ha descendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.07612533, shape=(), dtype=float32) tf.Tensor(0.9688, shape=(), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(yb,model(xb)), accuracy(model(xb),yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 784])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactorización usando tf.keras.layers.Dense \n",
    "\n",
    "Continuamos refactorizando nuestro código. En lugar de definir e inicializar manualmente *self.weights* y *self.bias*, y calcular *xb @ self.weights + self.bias*, en su lugar usaremos la clase TensorFlow *tf.keras.layers.Dense* para una capa lineal, que hace todo eso por nosotros. TensorFlow tiene muchos tipos de capas predefinidas que pueden simplificar enormemente nuestro código y, a menudo, también lo hacen más rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Mnist_Logistic,self).__init__()\n",
    "        self.linear = tf.keras.layers.Dense(10, input_shape=(None,784))\n",
    "        \n",
    "    def call(self, xb):\n",
    "        return self.linear(xb)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La clase tf.keras.layers.Dense\n",
    "\n",
    "La clase  [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) se deriva de la clase [tf.keras.layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer). Cuando se crea una instancia de clase, se reserva el espacio y se preparan todos los objetos, funciones requeridos para implementar una capa de la red. \n",
    "\n",
    "En momento de ejecución la instancia recibe un tensor $(N\\times D)$, en donde $N$ es el tamaño del batch y $D$ el tamaño de la entrada. En nuestro ejemplo, $(32 \\times 784)$. A la salida de la capa entrega otro tensor de tamaño $N\\times M$, en donde  $N$ es el tamaño del batch y $M$ el tamaño de la salida de la capa. En el ejemplo $(32 \\times 10)$.\n",
    "\n",
    "La instancia mantiene sus parámetros $\\mathbf{W}$ y $\\mathbf{b}$, los cuales son actualizados en el proceso de entrenamiento. Sin embargo, para que sean actualizados, los parámetros debe declararse como *trainable*, lo cual ocurre por defecto. \n",
    "\n",
    "Adicionalmente, es posible definir una función de activación para la capa y procedimientos de regularización.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Intanciar un objeto de clase  Mnist_Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.4284055, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_Logistic()\n",
    "print(loss_func(yb, model(xb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aun podemos usar el método fit como antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.10065323, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "fit()\n",
    "\n",
    "print(loss_func(yb, model(xb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactorización usando tf.keras.optimizers \n",
    "\n",
    "TensorFlow también tiene un paquete con varios algoritmos de optimización, *tf.keras.optimizers*. Podemos usar el método *apply_gradients* de nuestro optimizador para avanzar, en lugar de actualizar manualmente cada parámetro.\n",
    "\n",
    "Reeeplazaremos el fragmento de código\n",
    "\n",
    "    for variable, grad in zip(model.trainable_variables, gradients):\n",
    "        variable.assign_sub(lr * grad)\n",
    "\n",
    "por\n",
    "\n",
    "    opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    " \n",
    " Usaremos stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.4564323, shape=(), dtype=float32)\n",
      "tf.Tensor(0.10458888, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, tf.keras.optimizers.SGD(lr=lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "print(loss_func(yb,model(xb)))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1) // bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i   = start_i + bs\n",
    "        xb  = x_train[start_i:end_i]\n",
    "        yb  = y_train[start_i:end_i]\n",
    "        \n",
    "        with tf.GradientTape() as t:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(yb, pred)\n",
    "        gradients = t.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "\n",
    "print(loss_func(yb,model(xb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactorización usando Dataset\n",
    "\n",
    "TensorFlow proporciona la clase *tf.data.Dataset* como una abstracción sobre los datos necesarios para las tuberías de aprendizaje automático. Una manera simple de construir un conjunto de datos es a partir de los tensores existentes. El conjunto de datos puede contener tanto la entrada como las etiquetas, y también proporciona una manera fácil de iterar sobre lotes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((784,), ()), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previamente hemos iterado los lotes (batches) de datos usando el código\n",
    "\n",
    "    for i in range((n-1) // bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i   = start_i + bs\n",
    "        xb  = x_train[start_i:end_i]\n",
    "        yb  = y_train[start_i:end_i]\n",
    "\n",
    "Ahora lo reescribiremos como\n",
    "\n",
    "    for xb, yb in train_ds.batch(bs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.099200204, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_ds.batch(bs):\n",
    "        with tf.GradientTape() as t:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(yb,pred)\n",
    "        gradients = t.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "print(loss_func(yb, model(xb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gracias a *tf.keras.Model*, *tf.keras.optimizers*, y *tf.data.Dataset*, nuestro código ahora es dramáticamente más corto y fácil de entender que al comienzo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregando Validación\n",
    "\n",
    "Desde el comienzo hemos reservado un conjunto de datos para validar nuestro modelo: x_valid y y_valid, con el propósito de identificar sobreajuste (overfitting)\n",
    "\n",
    "Mezclar (shuffling) los datos de entrenamiento es importante para evitar la correlación entre lotes y sobreajuste. Por otro lado, la pérdida de validación será idéntica si barajamos  el conjunto de validación o no. Como barajar toma tiempo adicional, no tiene sentido barajar los datos de validación.\n",
    "\n",
    "Utilizaremos un tamaño de lote para el conjunto de validación que es dos veces más grande que el del conjunto de entrenamiento. Esto se debe a que el conjunto de validación no necesita propagación hacia atrás y, por lo tanto, requiere menos memoria (no necesita almacenar los gradientes). Aprovechamos esto para usar un tamaño de lote más grande y calcular la pérdida más rápidamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(SHUFFLE_BUFFER_SIZE).batch(bs)\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((x_valid,y_valid)).batch(bs*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcularemos e imprimiremos la pérdida y precisión de entrenamiento y validación al final de cada periodo (epoch). Para hacer esto, aprovecharemos el módulo *tf.keras.metrics*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.41274842619895935, Accuracy: 88.6449966430664, Valid Loss: 0.3169179856777191, Valid Accuracy: 90.76000213623047\n",
      "Epoch 2, Loss: 0.31246238946914673, Accuracy: 91.19667053222656, Valid Loss: 0.29630282521247864, Valid Accuracy: 91.36000061035156\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "validation_loss = tf.keras.metrics.Mean(name='validation_loss')\n",
    "validation_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='validation_accuracy')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_ds:\n",
    "        with tf.GradientTape() as t:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(yb,pred)\n",
    "            \n",
    "            gradients = t.gradient(loss, model.trainable_variables)\n",
    "            opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "            train_loss(loss)\n",
    "            train_accuracy(yb, pred)\n",
    "    \n",
    "    for xb, yb in validation_ds:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(yb, pred)\n",
    "        \n",
    "        validation_loss(loss)\n",
    "        validation_accuracy(yb, pred)\n",
    "        \n",
    "    # From https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Valid Loss: {}, Valid Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         train_accuracy.result()*100,\n",
    "                         validation_loss.result(),\n",
    "                         validation_accuracy.result()*100))\n",
    "    \n",
    "    # reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    validation_loss.reset_states()\n",
    "    validation_accuracy.reset_states()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejorando la función fit()\n",
    "\n",
    "Ahora haremos una pequeña refactorización propia. Dado que pasamos por un proceso similar dos veces de calcular la pérdida tanto para el conjunto de entrenamiento como para el conjunto de validación, hagámoslo en su propia función, *loss_batch*, que calcula la pérdida para un lote.\n",
    "\n",
    "Pasamos un optimizador para el conjunto de entrenamiento y lo usamos para realizar backprop. Para el conjunto de validación, no pasamos un optimizador, por lo que el método no realiza backprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, metric_loss, metric_accuracy, opt=None):\n",
    "    with tf.GradientTape() as t:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(yb,pred)\n",
    "        \n",
    "    if opt is not None:\n",
    "        gradients = t.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "        \n",
    "    metric_loss(loss)\n",
    "    metric_accuracy(yb, pred)\n",
    "    return loss, len(xb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la función fit queda ahora asi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_ds, valid_ds):\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuray = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    \n",
    "    validation_loss = tf.keras.metrics.Mean(name='validation_loss')\n",
    "    validation_accuray = tf.keras.metrics.SparseCategoricalAccuracy(name='validation_accuracy')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_ds:\n",
    "            loss_batch(model, loss_func, xb, yb, train_loss, train_accuracy, opt)\n",
    "         \n",
    "        for xb, yb in valid_ds:\n",
    "            loss_batch(model, loss_func, xb, yb, validation_loss, validation_accuracy)\n",
    "        \n",
    "        template = 'Epoch {}, Loss: {}, Accuracy: {}, Valid Loss: {}, Valid Accuracy: {}'\n",
    "        print(template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         train_accuracy.result()*100,\n",
    "                         validation_loss.result(),\n",
    "                         validation_accuracy.result()*100))\n",
    "    \n",
    "        # reset the metrics for the next epoch\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        validation_loss.reset_states()\n",
    "        validation_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora corremos todo el proceso de ajuste (fitting) del modelo. Solamente requerimos dos líneas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.41464585065841675, Accuracy: 88.6300048828125, Valid Loss: 0.31581640243530273, Valid Accuracy: 90.81999969482422\n",
      "Epoch 2, Loss: 0.312631756067276, Accuracy: 91.19833374023438, Valid Loss: 0.3005439043045044, Valid Accuracy: 91.29999542236328\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_ds, validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambiando a una red neuronal convolucionada (CNN)\n",
    "\n",
    "Ahora vamos a construir nuestra red neuronal con una capa convolucional. Debido a que ninguna de las funciones en la sección anterior asume nada sobre la forma del modelo, podremos usarlas para entrenar un CNN sin ninguna modificación.\n",
    "\n",
    "Utilizaremos la clase *tf.keras.layers.Conv2D* predefinida de TensorFlow como nuestra capa convolucional. Definimos un CNN con 1 capa convolucional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Mnist_CNN, self).__init__()\n",
    "        self.reshape1 = tf.keras.layers.Reshape((28,28,1), input_shape=((784,)))\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, activation='relu')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "        \n",
    "    def call(self, xb):\n",
    "        xb = self.reshape1(xb)\n",
    "        xb = self.conv1(xb)\n",
    "        xb = self.flatten(xb)\n",
    "        xb = self.d1(xb)\n",
    "        return self.d2(xb)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.21128425002098083, Accuracy: 94.02666473388672, Valid Loss: 0.09057081490755081, Valid Accuracy: 97.13999938964844\n",
      "Epoch 2, Loss: 0.06816711276769638, Accuracy: 97.92500305175781, Valid Loss: 0.06372962146997452, Valid Accuracy: 97.88999938964844\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_CNN()\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_ds, validation_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.Sequential\n",
    "\n",
    "*tf.keras* tiene otra clase útil que podemos usar para simplemente nuestro código *tf.keras.Sequential*. Un objeto secuencial ejecuta cada uno de los módulos contenidos en él, de manera secuencial. Esta es una forma más simple de escribir nuestra red neuronal.\n",
    "\n",
    "El modelo creado con secuencial es simplemente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.20524853467941284, Accuracy: 94.13833618164062, Valid Loss: 0.09728005528450012, Valid Accuracy: 96.80000305175781\n",
      "Epoch 2, Loss: 0.06621823459863663, Accuracy: 98.0, Valid Loss: 0.06874193996191025, Valid Accuracy: 97.79000091552734\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Reshape((28,28,1), input_shape=((784,))),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_ds, validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando la función pre-construida fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Reshape((28,28,1), input_shape=((784,))),\n",
    "    tf.keras.layers.Conv2D(32,kernel_size=3, strides = 2, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10,activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 938 steps\n",
      "Epoch 1/2\n",
      "938/938 [==============================] - 25s 26ms/step - loss: 0.2256 - accuracy: 0.9350\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 57s 60ms/step - loss: 0.0728 - accuracy: 0.9777\n",
      "79/79 [==============================] - 3s 40ms/step - loss: 0.0626 - accuracy: 0.9791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0625824541048017, 0.9791]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss ='sparse_categorical_crossentropy',\n",
    "              metrics =['accuracy'])\n",
    "model.fit(train_ds, epochs =2)\n",
    "model.evaluate(validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
