{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title> <h1><center><span class=\"header-section-number\"> 100. </span> Tensorflow 2.0.<br />\n",
    "¿Qué es tf.keras realmente?<./h1></center></title>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 100.1 </span> Introducción </h2>\n",
    "\n",
    "Este tutorial es una traducción del tutorial <a href=\"https://jaredwinick.github.io/what_is_tf_keras/\">What is tf.keras really?</a> Todos los créditos deben ser dados a All  Jeremy Howard  y sus colaboradores Rachel Thomas and Francisco Ingham. Algunos modificaciones menores fueron hechas. La lectura de datos no la haremos directamente de la URL usada por los autores, porque el archivo fue movido de allí. Usaremos los datos disponible en *tf.keras.dataset*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 100.1 </span> La base de datos MNIST </h2>\n",
    "\n",
    "Utilizaremos el clásico conjunto de datos MNIST, que consiste en imágenes en blanco y negro de dígitos dibujados a mano (entre 0 y 9).\n",
    "\n",
    "Para empezar importamos las librerías requeridas y se leen los datos de la base de datos. Luego se muestra la primera imágen de la base de datos usando las herramientas de Python. Cada imagen está almacenada en tamaño 28x28. son 60,000 imágenes de entrenamiento y 10,000 de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de Tensorflow:  2.0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Versión de Tensorflow: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape x_train =  (60000, 784)\n",
      "type x_train =  <dtype: 'float32'>\n",
      "Min x_train =  0.0\n",
      "Max x_train =  1.0\n",
      "type y_train =  <dtype: 'int32'>\n",
      "Min y_train =  0\n",
      "Max y_train =  9\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_valid, y_valid) = tf.keras.datasets.mnist.load_data()\n",
    "# resize x\n",
    "x_train.resize((60000,784)) # Change shape and size of array in-place.\n",
    "x_valid.resize((10000,784))\n",
    "\n",
    "\n",
    "# transform x\n",
    "x_train = tf.constant(x_train/255., dtype= tf.float32)\n",
    "x_valid = tf.constant(x_valid/255., dtype= tf.float32)\n",
    "print(\"shape x_train = \",x_train.shape)\n",
    "print(\"type x_train = \",x_train.dtype)\n",
    "print(\"Min x_train = \",tf.reduce_min(x_train).numpy())\n",
    "print(\"Max x_train = \",tf.reduce_max(x_train).numpy())\n",
    "\n",
    "# cast y\n",
    "y_train = tf.constant(y_train,dtype=tf.int32)\n",
    "y_valid = tf.constant(y_valid,dtype=tf.int32)\n",
    "print(\"type y_train = \",y_train.dtype)\n",
    "print(\"Min y_train = \",tf.reduce_min(y_train).numpy())\n",
    "print(\"Max y_train = \",tf.reduce_max(y_train).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(x_train[0].numpy().reshape((28,28)),cmap=\"gray\")\n",
    "plt.show()\n",
    "print(x_train.shape)\n",
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos datos tiene formato de arreglos numpy. Tensorflow utiliza el formato tf.Tensor, por lo que es necesario transformar los datos a dicho formato.  Usaremos la función *tf.Variable* para hacer la conversión. Note que se ha dividido x_train y x_valid entre 255, para trasnfprmar los datos a escala $[0.0,1.0]$. Si los datos no son transformados use *tf.constant*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 100.2 </span> La Red Neuronal desde cero (sin tf.keras) </h2>\n",
    "\n",
    "Vamos a crear primero un modelo, para lo cual solamente usaremos las operacoones de Tensorflow.  Los pesos son inicializdos siguiendo la propuesta de <a href='http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf'>Gorot_Bengio </a>. Se generaran números aleatorios y se divide por $\\sqrt{n}$. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "weights = tf.Variable(tf.random.normal((784,10))/math.sqrt(784), dtype= tf.float32)\n",
    "bias = tf.Variable(tf.zeros(10),tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota.** Tensorflow 2.0 incluye **diferenciación automática** para calcular gradientes automáticamente. Esto permite usar funciones estándar de Python como modelos para la redes. Vamos a crear un modeo lineal simple. Necesitaremos una función de activación, por lo que vamos a escribir la función *log_softmax*. La variable *eta* en la función, que corresponde al máximo de los valores, se introduce para tener una función muy robusta, desde el punto de vista computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    eta = tf.math.reduce_max(x)\n",
    "    return x -  tf.math.log(tf.math.reduce_sum(tf.math.exp(x), -1, keepdims=True))\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda anterior, el símbolo @ representa la operación del producto punto. Llamaremos a nuestra función con un lote de datos (en este caso, 64 imágenes). Este es un pase hacia adelante (forward pass). Tenga en cuenta que nuestras predicciones no serán mejores que aleatorias en esta etapa, ya que comenzamos con pesos aleatorios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-2.314689  -2.8350103 -2.671541  -2.3839228 -2.3984456 -2.5152397\n",
      " -1.8988264 -2.2424781 -1.962054  -2.1808972], shape=(10,), dtype=float32) (64, 10)\n"
     ]
    }
   ],
   "source": [
    "bs = 64 # batch size\n",
    "\n",
    "xb = x_train[0:bs]\n",
    "preds = model(xb) # predictions\n",
    "print(preds[0], preds.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a implementar la función de log-verosimilitud negativa, la cual usaremos como **función de pérdida** (loss function). Esta es la misma **entropía cruzada**(cross entropy, usando la codificación One-hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    indices = tf.stack([tf.range(input.shape[0], dtype=tf.int32),target], axis=1)\n",
    "    return tf.math.reduce_mean(-tf.gather_nd(input, indices))\n",
    "\n",
    "loss_func = nll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre el código anterior. \n",
    "1. *tf.range(input.shape[0])*  genera un tensor con números en el rango entre 0 1 63, porque la forma (shape) es 64, el tamaño del batch.\n",
    "2. *tf.stack([tf.range(input.shape[0], dtype=tf.int64),target], axis=1)* crea un tensor bidimensional, en donde cada fila corresponde al índice del elemento que debe tomarse de cada fila de input para calcualr la funcion de párdida. Por ejemplo, si la fila 3 de índice es [3,4], significará que y_train[3] es 4, y ṕor tanto se requiere tomar el contenido de la cuarta posición de la entrada 3.\n",
    "3. tf.gather_nd(input, indices). Para cada fila de input toma el valor de input en la posición indicada en el índice, como se explica en el numeral 2.\n",
    "4. tf.math.reduce_mean(-tf.gather_nd(input, indices)), calcula el resumen. La media, que corresponde exáctamete a - log likelihood\n",
    "\n",
    "Revisamos ahora la **función de pérdida** para el modelo con datos aleatorios. Esto permitirá comparar las mejorsa después del paso de propagacion hacia atrás  (backpropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.434606, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds,yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También implementamos la función precisión (accuracy) de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out,yb):\n",
    "    preds = tf.dtypes.cast(tf.math.argmax(out,axis=1), tf.int32)   # posición del valor máximo en cada arreglo\n",
    "    return tf.math.reduce_mean(tf.dtypes.cast(preds==yb,tf.float16))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chequamos la precisión actual del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.1562, shape=(), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(preds,yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Estamos listos para correr el ciclo de entranamiento. Para calcular los gradientes usaremos diferenciación automática, con la clase tf.GradientTape.\n",
    " \n",
    "- Seleccionamos un mini-batch de datos de tamaño *bs*\n",
    "- Bajo un contexto tf.GradientTape \n",
    " - Usamos el modelo para hacer predicciones\n",
    " - Calculamos la pérdida\n",
    "- Calculamos el gradientes de las operaciones registradas en el contexto de esta cinta (tape).\n",
    "\n",
    "Ahora usamos estos gradientes para actualizar los pesos (weights) y los desplazamientos (bias).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.2 # learning rate\n",
    "epochs = 2 # how many epochs to train for\n",
    "n = x_train.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs +1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        with tf.GradientTape() as t:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred,yb)\n",
    "        dW, dB = t.gradient(loss,[weights,bias])\n",
    "        weights.assign_sub(lr*dW)\n",
    "        bias.assign_sub(lr*dB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos creado y entrenado una red neuronal minimal( en este caso una regression logística, dado que no tiene capas ocultas) totalmwente desde cero (from scratch).\n",
    "\n",
    "Revisemos la pérdida y precisión del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.09764923, shape=(), dtype=float32)\n",
      "tf.Tensor(0.9688, shape=(), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb),yb))\n",
    "print(accuracy(model(xb),yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 100.2 </span> La Red Neuronal usando tf.keras</h2>\n",
    "\n",
    "Ahora refactorizaremos nuestro código, para que haga lo mismo que antes, solo que comenzaremos a aprovechar las clases *tf.keras* de TensorFlow para hacerlo más conciso y flexible. En cada paso a partir de aquí, deberíamos hacer que nuestro código sea más corto, más comprensible y / o más flexible.\n",
    "\n",
    "El primer y más fácil paso es acortar nuestro código reemplazando nuestras funciones de activación y pérdida escritas a mano con las de tf.keras.\n",
    "\n",
    "TensorFlow proporciona una función única función *tf.keras.losses.SparseCategoricalCrossentropy* que combina una activación softmax con una función de pérdida. Tenga en cuenta que usamos from_logit = True aquí porque no estamos pasando una distribución de probabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.09764923, shape=(), dtype=float32) tf.Tensor(0.9688, shape=(), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(yb,model(xb)), accuracy(model(xb),yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 100.3 </span> Refactorización usando tf.keras.Model </h2>\n",
    "\n",
    "A continuación, utilizaremos *tf.keras.Model* para un ciclo de entrenamiento más claro y conciso. Hacemos una subclase (heredamos) de la clase *tf.keras.Model* (que en sí mismo es una clase y puede realizar un seguimiento del estado). En este caso, queremos crear una clase que contenga nuestros pesos, sesgos y métodos para el paso adelante (forward pass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_logistic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Mnist_logistic,self).__init__()\n",
    "        self.w = tf.Variable(tf.random.normal((784,10)) /math.sqrt(784))\n",
    "        self.b = tf.Variable(tf.zeros(10))\n",
    "    \n",
    "    def call(self, xb):\n",
    "        return xb @ self.w + self.b\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ahora estamos usando un objeto en lugar de solo usar una función, primero tenemos que instanciar nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos calcular la pérdida de la misma manera que antes. Tenga en cuenta que los objetos *tf.keras.Model* se usan como si fueran funciones (es decir, son invocables), pero detrás de escena TensorFlow llamará a nuestro método *call* automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.2851243, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(yb,model(xb)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Anteriormente para nuestro ciclo de entrenamiento teníamos que actualizar los valores para cada parámetro por nombre, de esta manera:\n",
    "\n",
    "    weights.assign_sub (lr * dW)\n",
    "    bias.assign_sub (lr * dB)\n",
    "\n",
    "\n",
    "Ahora podemos aprovechar *model.trainable_variables* para hacer que esos pasos sean más concisos y menos propensos al error de olvidar algunos de nuestros parámetros, especialmente si teníamos un modelo más complicado.\n",
    "\n",
    "Envolveremos nuestro pequeño bucle de entrenamiento en una función de ajuste para poder ejecutarlo nuevamente más tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1)//bs +1):\n",
    "            start_i = i*bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            with tf.GradientTape() as t:\n",
    "                pred = model(xb)\n",
    "                loss = loss_func(yb, pred)\n",
    "                \n",
    "            gradients = t.gradient(loss, model.trainable_variables)\n",
    "            for variable, grad in zip(model.trainable_variables, gradients):\n",
    "                variable.assign_sub(lr * grad)\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chequeemos que la  pérdida ha descendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.06662762, shape=(), dtype=float32) tf.Tensor(0.9688, shape=(), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(yb,model(xb)), accuracy(model(xb),yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 784])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 100.3 </span> Refactorización usando tf.keras.layers.Dense </h2>\n",
    "\n",
    "Continuamos refactorizando nuestro código. En lugar de definir e inicializar manualmente *self.weights* y *self.bias*, y calcular *xb @ self.weights + self.bias*, en su lugar usaremos la clase TensorFlow *tf.keras.layers.Dense* para una capa lineal, que hace todo eso por nosotros. TensorFlow tiene muchos tipos de capas predefinidas que pueden simplificar enormemente nuestro código y, a menudo, también lo hacen más rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Mnist_Logistic,self).__init__()\n",
    "        self.linear = tf.keras.layers.Dense(10, input_shape=(None,784))\n",
    "        \n",
    "    def call(self, xb):\n",
    "        return self.linear(xb)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos el modelo y calculamos la pérdida inicial, como antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.36521, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_Logistic()\n",
    "print(loss_func(yb, model(xb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aun podemos usar el método fit como antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.09545911, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "fit()\n",
    "\n",
    "print(loss_func(yb, model(xb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 100.3 </span> Refactorización usando tf.keras.optimizers </h2>\n",
    "\n",
    "TensorFlow también tiene un paquete con varios algoritmos de optimización, *tf.keras.optimizers*. Podemos usar el método *apply_gradients* de nuestro optimizador para avanzar, en lugar de actualizar manualmente cada parámetro.\n",
    "\n",
    "Reeeplazaremos el fragmento de código\n",
    "\n",
    "    for variable, grad in zip(model.trainable_variables, gradients):\n",
    "        variable.assign_sub(lr * grad)\n",
    "\n",
    "por\n",
    "\n",
    "    opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    " \n",
    " Usaremos stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.3960228, shape=(), dtype=float32)\n",
      "tf.Tensor(0.09480931, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, tf.keras.optimizers.SGD(lr=lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "print(loss_func(yb,model(xb)))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1) // bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i   = start_i + bs\n",
    "        xb  = x_train[start_i:end_i]\n",
    "        yb  = y_train[start_i:end_i]\n",
    "        \n",
    "        with tf.GradientTape() as t:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(yb, pred)\n",
    "        gradients = t.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "\n",
    "print(loss_func(yb,model(xb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 100.4 </span> Refactorización usando Dataset</h2>\n",
    "\n",
    "TensorFlow proporciona la clase *tf.data.Dataset* como una abstracción sobre los datos necesarios para las tuberías de aprendizaje automático. Una manera simple de construir un conjunto de datos es a partir de los tensores existentes. El conjunto de datos puede contener tanto la entrada como las etiquetas, y también proporciona una manera fácil de iterar sobre lotes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previamente hemos itrado los lotes (batches) de datos usando el código\n",
    "\n",
    "    for i in range((n-1) // bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i   = start_i + bs\n",
    "        xb  = x_train[start_i:end_i]\n",
    "        yb  = y_train[start_i:end_i]\n",
    "\n",
    "Ahora lo reescribiremos como\n",
    "\n",
    "    for xb, yb in train_ds.batch(bs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.10288866, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_ds.batch(bs):\n",
    "        with tf.GradientTape() as t:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(yb,pred)\n",
    "        gradients = t.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "print(loss_func(yb, model(xb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gracias a *tf.keras.Model*, *tf.keras.optimizers*, y *tf.data.Dataset*, nuestro código ahora es dramáticamente más corto y fácil de entender que al comienzo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 100.5 </span> Agregando Validación</h2>\n",
    "\n",
    "Desde el comienzo hemos reservado un conjunto de datos para validar nuestro modelo: x_valid y y_valid, con el propósito de identificar sobreajuste (overfitting)\n",
    "\n",
    "Mezclar (shuffling) los datos de entrenamiento es importante para evitar la correlación entre lotes y sobreajuste. Por otro lado, la pérdida de validación será idéntica si barajamos (we shuffle) el conjunto de validación o no. Como barajar toma tiempo adicional, no tiene sentido barajar los datos de validación.\n",
    "\n",
    "Utilizaremos un tamaño de lote para el conjunto de validación que es dos veces más grande que el del conjunto de entrenamiento. Esto se debe a que el conjunto de validación no necesita propagación hacia atrás y, por lo tanto, requiere menos memoria (no necesita almacenar los gradientes). Aprovechamos esto para usar un tamaño de lote más grande y calcular la pérdida más rápidamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(SHUFFLE_BUFFER_SIZE).batch(bs)\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((x_valid,y_valid)).batch(bs*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcularemos e imprimiremos la pérdida y precisión de entrenamiento y validación al final de cada periodo (epoch). Para hacer esto, aprovecharemos el módulo *tf.keras.metrics*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4139716625213623, Accuracy: 88.62166595458984, Valid Loss: 0.32240140438079834, Valid Accuracy: 90.51000213623047\n",
      "Epoch 2, Loss: 0.3126678168773651, Accuracy: 91.23999786376953, Valid Loss: 0.2951362431049347, Valid Accuracy: 91.57999420166016\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "validation_loss = tf.keras.metrics.Mean(name='validation_loss')\n",
    "validation_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='validation_accuracy')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_ds:\n",
    "        with tf.GradientTape() as t:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(yb,pred)\n",
    "            \n",
    "            gradients = t.gradient(loss, model.trainable_variables)\n",
    "            opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "            train_loss(loss)\n",
    "            train_accuracy(yb, pred)\n",
    "    \n",
    "    for xb, yb in validation_ds:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(yb, pred)\n",
    "        \n",
    "        validation_loss(loss)\n",
    "        validation_accuracy(yb, pred)\n",
    "        \n",
    "    # From https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Valid Loss: {}, Valid Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         train_accuracy.result()*100,\n",
    "                         validation_loss.result(),\n",
    "                         validation_accuracy.result()*100))\n",
    "    \n",
    "    # reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    validation_loss.reset_states()\n",
    "    validation_accuracy.reset_states()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 100.6 </span> Mejorando la función fit()</h2>\n",
    "\n",
    "Ahora haremos una pequeña refactorización propia. Dado que pasamos por un proceso similar dos veces de calcular la pérdida tanto para el conjunto de entrenamiento como para el conjunto de validación, hagámoslo en su propia función, *loss_batch*, que calcula la pérdida para un lote.\n",
    "\n",
    "Pasamos un optimizador para el conjunto de entrenamiento y lo usamos para realizar backprop. Para el conjunto de validación, no pasamos un optimizador, por lo que el método no realiza backprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, metric_loss, metric_accuracy, opt=None):\n",
    "    with tf.GradientTape() as t:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(yb,pred)\n",
    "        \n",
    "    if opt is not None:\n",
    "        gradients = t.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "        \n",
    "    metric_loss(loss)\n",
    "    metric_accuracy(yb, pred)\n",
    "    return loss, len(xb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la función fit queda ahora asi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_ds, valid_ds):\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuray = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    \n",
    "    validation_loss = tf.keras.metrics.Mean(name='validation_loss')\n",
    "    validation_accuray = tf.keras.metrics.SparseCategoricalAccuracy(name='validation_accuracy')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_ds:\n",
    "            loss_batch(model, loss_func, xb, yb, train_loss, train_accuracy, opt)\n",
    "         \n",
    "        for xb, yb in valid_ds:\n",
    "            loss_batch(model, loss_func, xb, yb, validation_loss, validation_accuracy)\n",
    "        \n",
    "        template = 'Epoch {}, Loss: {}, Accuracy: {}, Valid Loss: {}, Valid Accuracy: {}'\n",
    "        print(template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         train_accuracy.result()*100,\n",
    "                         validation_loss.result(),\n",
    "                         validation_accuracy.result()*100))\n",
    "    \n",
    "        # reset the metrics for the next epoch\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        validation_loss.reset_states()\n",
    "        validation_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora corremos todo el proceso de ajuste (fitting) del modelo. Solamente requerimos dos líneas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.41325464844703674, Accuracy: 88.68000030517578, Valid Loss: 0.3169042468070984, Valid Accuracy: 91.0199966430664\n",
      "Epoch 2, Loss: 0.3123631477355957, Accuracy: 91.28999328613281, Valid Loss: 0.2999750077724457, Valid Accuracy: 91.32999420166016\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_ds, validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 100.7 </span> Cambiando a una red neuronal convolucionada (CNN)</h2>\n",
    "\n",
    "Ahora vamos a construir nuestra red neuronal con una capa convolucional. Debido a que ninguna de las funciones en la sección anterior asume nada sobre la forma del modelo, podremos usarlas para entrenar un CNN sin ninguna modificación.\n",
    "\n",
    "Utilizaremos la clase *tf.keras.layers.Conv2D* predefinida de TensorFlow como nuestra capa convolucional. Definimos un CNN con 1 capa convolucional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Mnist_CNN, self).__init__()\n",
    "        self.reshape1 = tf.keras.layers.Reshape((28,28,1), input_shape=((784,)))\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, activation='relu')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "        \n",
    "    def call(self, xb):\n",
    "        xb = self.reshape1(xb)\n",
    "        xb = self.conv1(xb)\n",
    "        xb = self.flatten(xb)\n",
    "        xb = self.d1(xb)\n",
    "        return self.d2(xb)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.21882613003253937, Accuracy: 93.76166534423828, Valid Loss: 0.10195889323949814, Valid Accuracy: 96.66999816894531\n",
      "Epoch 2, Loss: 0.07251006364822388, Accuracy: 97.84667205810547, Valid Loss: 0.07027331739664078, Valid Accuracy: 97.73999786376953\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_CNN()\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_ds, validation_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 100.8 </span> tf.keras.Sequential</h2>\n",
    "\n",
    "*tf.keras* tiene otra clase útil que podemos usar para simplemente nuestro código *tf.keras.Sequential*. Un objeto secuencial ejecuta cada uno de los módulos contenidos en él, de manera secuencial. Esta es una forma más simple de escribir nuestra red neuronal.\n",
    "\n",
    "El modelo creado con secuencial es simplemente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.23225533962249756, Accuracy: 93.32666015625, Valid Loss: 0.11613459885120392, Valid Accuracy: 96.34000396728516\n",
      "Epoch 2, Loss: 0.07633589953184128, Accuracy: 97.66500091552734, Valid Loss: 0.06803903728723526, Valid Accuracy: 97.81999969482422\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Reshape((28,28,1), input_shape=((784,))),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_ds, validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\"> 100.9 </span> Usando la función pre-construida fit</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Reshape((28,28,1), input_shape=((784,))),\n",
    "    tf.keras.layers.Conv2D(32,kernel_size=3, strides = 2, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10,activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "938/938 [==============================] - 29s 31ms/step - loss: 0.2120 - accuracy: 0.9402\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 22s 23ms/step - loss: 0.0688 - accuracy: 0.9794\n",
      "79/79 [==============================] - 1s 16ms/step - loss: 0.0696 - accuracy: 0.9768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06963293598537464, 0.9768]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss ='sparse_categorical_crossentropy',\n",
    "              metrics =['accuracy'])\n",
    "model.fit(train_ds, epochs =2)\n",
    "model.evaluate(validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
