{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'><span class=\"header-section-number\"> </span>Diferenciación Automática con JAX <br/>Implementación de la función XOR</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.  Introducción </h2>\n",
    "\n",
    "Con su versión actualizada de [Autograd](https://github.com/hips/autograd), [JAX](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html) puede diferenciar automáticamente el código nativo de Python y NumPy. Puede derivarse través de un gran subconjunto de características de Python, incluidos bucles, ifs, recursión y clousures, e incluso puede tomar derivadas de derivadas de derivadas. Admite la diferenciación tanto en modo inverso como en modo directo, y los dos pueden componerse arbitrariamente en cualquier orden.\n",
    "\n",
    "Lo nuevo es que JAX usa [XLA](https://www.tensorflow.org/xla) para compilar y ejecutar su código NumPy en aceleradores, como GPU y TPU. La compilación ocurre de forma predeterminada, con las llamadas de la biblioteca compiladas y ejecutadas justo a tiempo. Pero JAX incluso le permite compilar justo a tiempo sus propias funciones de Python en núcleos optimizados para XLA utilizando una API de una función. La compilación y la diferenciación automática se pueden componer de forma arbitraria, por lo que puede expresar algoritmos sofisticados y obtener el máximo rendimiento sin tener que abandonar Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade jax jaxlib \n",
    "from __future__ import print_function\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "key = random.PRNGKey(0)\n",
    "# Current convention is to import original numpy as \"onp\"\n",
    "import numpy as onp\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "import jax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Función XOR </h2>\n",
    "\n",
    "En este documento implementamos una red neuronal que calcula la función XOR. Esta es una función muy famosa en la historia de las redes neuronales artificiales, dado que fué la causante del primer invierno de estas.\n",
    "\n",
    "La función lógica XOR es definida por\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(0,0) &= 1\\\\\n",
    "f(0,1) &= 0\\\\\n",
    "f(1,0) &=0\\\\\n",
    "f(0,0) &=1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Usaremos una red neuronal con una sola capa oculta con 3 neuronas y una no linealidad tangente hiperbólica, entrenada con la función de pérdida *entropía cruzada*, optimizando través del descenso de gradiente estocástico. Implementemos este modelo y la función de pérdida. Tenga en cuenta que el código es exactamente como lo escribiría en numpy estándar.\n",
    "<h2> 3. Funciones Requeridas </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the output activation\n",
    "def sigmoid(x):\n",
    "# more stable 0.5*(np.tanh(x/2)+1)\n",
    "    return 1.0/(1+np.exp(-x))\n",
    "\n",
    "# define the net\n",
    "def net(params, x):\n",
    "    w1, b1, w2, b2 = params\n",
    "    hidden = np.tanh(np.dot(w1,x) + b1)\n",
    "    return (sigmoid(np.dot(w2,hidden) + b2))\n",
    "\n",
    "# cross entropy loss function\n",
    "def loss(params, x,y):\n",
    "    out = net(params,x)\n",
    "    cross_entropy =  -y * np.log(out) - (1-y)*np.log(1-out) # esta es -log likelihood\n",
    "    return cross_entropy\n",
    "\n",
    "# Utility function for testing whether the net produces the correct\n",
    "# output for all possible inputs\n",
    "def test_all_inputs(inputs, params):\n",
    "    predictions = [int(net(params, inp) > 0.5) for inp in inputs]\n",
    "    for inp, out in zip(inputs, predictions):\n",
    "        print(inp, '->', out)\n",
    "    return (predictions == [onp.bitwise_xor(*inp) for inp in inputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay algunos lugares donde queremos usar numpy estándar en lugar de jax.numpy. Uno de esos lugares es con la inicialización de parámetros. Nos gustaría inicializar nuestros parámetros al azar antes de entrenar nuestra red, que no es una operación para la que necesitamos derivados o compilación. JAX usa su propia biblioteca jax.random en lugar de numpy.random que proporciona un mejor soporte para la reproducibilidad (siembra) a través de diferentes transformaciones. Dado que no necesitamos transformar la inicialización de los parámetros de ninguna manera, es más simple usar numpy.random estándar en lugar de jax.random aquí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4. jax.grad </h2>\n",
    "\n",
    "La primera transformación que usaremos es *jax.grad*. *jax.grad* toma una función y devuelve una nueva función que calcula el gradiente de la función original. Por defecto, el gradiente se toma con respecto al primer argumento; esto se puede controlar mediante el argumento argnums de jax.grad. Para usar el gradiente descen diente, queremos poder calcular el gradiente de nuestra función de pérdida con respecto a los parámetros de nuestra red neuronal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad = grad(loss)\n",
    "\n",
    "# Stochastic gradient descent\n",
    "# Learning rate\n",
    "learning_rate = 1.0\n",
    "# all possible inputs \n",
    "inputs  = onp.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "targets = onp.array([0,1,1,0])\n",
    "ide     =  onp.array([0,1,2,3])\n",
    "# Initialize parameters randomly\n",
    "params = initial_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.25280359, -1.05972091],\n",
       "        [-0.86363086, -0.57583006],\n",
       "        [-0.06159423,  0.98018524]]),\n",
       " array([1.37862389, 0.94909999, 1.6194938 ]),\n",
       " array([-0.14456356,  0.42562094, -0.34343166]),\n",
       " -0.6278622829106619]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration0\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 0\n",
      "[1 1] -> 0\n",
      "Iteration100\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 1\n",
      "Iteration200\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n"
     ]
    }
   ],
   "source": [
    "for n in itertools.count():\n",
    "    # grab a single random input\n",
    "    ix = ide[onp.random.choice(ide.shape[0])]\n",
    "    # input\n",
    "    x  = inputs[ix]\n",
    "    # output\n",
    "    y = targets[ix]\n",
    "    # get the gradient  of the loss for this input/output losss\n",
    "    grads = loss_grad(params,x,y)\n",
    "    # update parameters via gradient descent\n",
    "    params = [param - learning_rate * grad \n",
    "              for param, grad in zip(params,grads) ]\n",
    "    # Every 100 iterations, check whether we've solve XOR\n",
    "    if not n %100:\n",
    "        print('Iteration{}'.format(n))\n",
    "        if test_all_inputs(inputs, params):\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DeviceArray([[ 3.7044861, -2.387884 ],\n",
       "              [-2.489525 , -3.1954105],\n",
       "              [-2.625351 ,  3.7185783]], dtype=float32),\n",
       " DeviceArray([0.79461575, 0.5428428 , 1.2141498 ], dtype=float32),\n",
       " DeviceArray([-3.5103402, -2.874178 , -3.5018072], dtype=float32),\n",
       " DeviceArray(0.32925892, dtype=float32)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DeviceArray([[ 3.7044861, -2.387884 ],\n",
       "              [-2.489525 , -3.1954105],\n",
       "              [-2.625351 ,  3.7185783]], dtype=float32),\n",
       " DeviceArray([0.79461575, 0.5428428 , 1.2141498 ], dtype=float32),\n",
       " DeviceArray([-3.5103402, -2.874178 , -3.5018072], dtype=float32),\n",
       " DeviceArray(0.32925892, dtype=float32)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5. jax.jit </h2>\n",
    "\n",
    "Si bien el código numpy cuidadosamente escrito puede ser razonablemente eficaz, para el aprendizaje automático moderno queremos que nuestro código se ejecute lo más rápido posible. Esto a menudo implica ejecutar nuestro código en diferentes \"aceleradores\" como GPU o TPU. *JAX* proporciona un compilador *JIT* (justo a tiempo) que toma una función estándar de *Python/numpy* y la compila para ejecutarse eficientemente en un acelerador. Compilar una función también evita la sobrecarga del intérprete de Python, lo que ayuda tanto si está utilizando un acelerador como si no. En total, *jax.jit* puede acelerar drásticamente su código esencialmente sin sobrecarga de codificación; solo tiene que pedirle a JAX que compile la función por usted. Incluso nuestra pequeña red neuronal puede ver una aceleración bastante dramática al usar *jax.jit*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.8 ms ± 755 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "641 µs ± 124 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Time the original gradient function\n",
    "%timeit loss_grad(params, x, y)\n",
    "loss_grad = jax.jit(jax.grad(loss))\n",
    "# Run once to trigger JIT compilation\n",
    "loss_grad(params, x, y)\n",
    "%timeit loss_grad(params, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run again the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration0\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n"
     ]
    }
   ],
   "source": [
    "for n in itertools.count():\n",
    "    # grab a single random input\n",
    "    ix = ide[onp.random.choice(ide.shape[0])]\n",
    "    # input\n",
    "    x  = inputs[ix]\n",
    "    # output\n",
    "    y = targets[ix]\n",
    "    # get the gradient  of the loss for this input/output losss\n",
    "    grads = loss_grad(params,x,y)\n",
    "    # update parameters via gradient descent\n",
    "    params = [param - learning_rate * grad \n",
    "              for param, grad in zip(params,grads) ]\n",
    "    # Every 100 iterations, check whether we've solve XOR\n",
    "    if not n %100:\n",
    "        print('Iteration{}'.format(n))\n",
    "        if test_all_inputs(inputs, params):\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 6.  jax.vmap </h2>\n",
    "\n",
    "Hemos estado entrenando nuestra red neuronal en un solo ejemplo a la vez. Este es el \"verdadero\" descenso de gradiente estocástico; en la práctica, cuando entrenamos modelos modernos de aprendizaje automático, realizamos un descenso de gradiente “minibatch” donde promediamos los gradientes de pérdida en un mini lote de ejemplos en cada paso del descenso de gradiente. \n",
    "\n",
    "*JAX* proporciona *jax.vmap*, que es una transformación que automáticamente \"vectoriza\" una función. Lo que esto significa es que le permite calcular la salida de una función en paralelo sobre algún eje de la entrada. Para nosotros, esto significa que podemos aplicar la transformación de la función *jax.vmap* e inmediatamente obtener una versión de nuestro gradiente de la función de pérdida que es susceptible de utilizar un minibatch de ejemplos.\n",
    "\n",
    "*jax.vmap* toma argumentos adicionales:\n",
    "\n",
    "- *in_axes* es una tupla o número entero que le dice a *JAX* sobre qué ejes deben paralelizarse los argumentos de la función. La tupla debe tener la misma longitud que el número de argumentos de la función que se está vectorizando, o debe ser un número entero cuando solo hay un argumento. En nuestro ejemplo, usaremos *(None, 0, 0)*, que significa \"no paralelizar sobre el primer argumento (parámetros), y paralelizar sobre la primera dimensión (cero) del segundo y tercer argumento (x e y) \".\n",
    "- *out_axes* es análogo a in_axes, excepto que especifica qué ejes de la salida de la función se deben paralelizar. En nuestro caso, usaremos 0, que significa paralelizar sobre la primera dimensión (cero) de la única salida de la función (los gradientes de pérdida).\n",
    "\n",
    "Tenga en cuenta que tendremos que cambiar un poco el código de entrenamiento: necesitamos obtener un lote de datos en lugar de un solo ejemplo a la vez, y debemos promediar los gradientes sobre el lote antes de aplicarlos para actualizar los parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 0\n",
      "[1 1] -> 0\n",
      "Iteration 100\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n"
     ]
    }
   ],
   "source": [
    "loss_grad = jax.jit(jax.vmap(jax.grad(loss), in_axes=(None, 0, 0), out_axes=0))\n",
    "\n",
    "params = initial_params()\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for n in itertools.count():\n",
    "    # Generate a batch of inputs\n",
    "    x = inputs[onp.random.choice(inputs.shape[0], size=batch_size)]\n",
    "    y = onp.bitwise_xor(x[:, 0], x[:, 1])\n",
    "    # The call to loss_grad remains the same!\n",
    "    grads = loss_grad(params, x, y)\n",
    "    # Note that we now need to average gradients over the batch\n",
    "    params = [param - learning_rate * np.mean(grad, axis=0)\n",
    "              for param, grad in zip(params, grads)]\n",
    "    if not n % 100:\n",
    "        print('Iteration {}'.format(n))\n",
    "        if test_all_inputs(inputs, params):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DeviceArray([[-2.7300384 ,  2.8734598 ],\n",
       "              [ 0.0461734 ,  0.20690341],\n",
       "              [-2.25555   ,  2.1526465 ]], dtype=float32),\n",
       " DeviceArray([ 1.4270214,  1.1485996, -1.0554334], dtype=float32),\n",
       " DeviceArray([-3.0328572,  1.0033779,  3.0124404], dtype=float32),\n",
       " DeviceArray(1.8351157, dtype=float32)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
